<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Models on Chris&#39;s Data Blog</title>
    <link>/tags/models/</link>
    <description>Recent content in Models on Chris&#39;s Data Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 Apr 2019 10:58:08 -0400</lastBuildDate>
    
	<atom:link href="/tags/models/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Predicting Gas Guzzlers using the Neuralnet R Package</title>
      <link>/post/neuralnet/</link>
      <pubDate>Sat, 06 Apr 2019 10:58:08 -0400</pubDate>
      
      <guid>/post/neuralnet/</guid>
      <description>Objective:To predict weather a car in the mtcars dataset is a gas guzzler or not.
Look at the distribution of mpg in the dataset by drawing a quick histogram.
Create a binary variable (1 or 0) which we will use as our Output (similar to an independent variable).Split dataset into a Training and Test set.Train Neural Network on training set.Test the Neural Network on the test set.</description>
    </item>
    
    <item>
      <title>Prevent MultiCollinearity &#43; Visualize more than 3 variables at one time.</title>
      <link>/post/pci/pci_analysis/</link>
      <pubDate>Fri, 21 Dec 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/pci/pci_analysis/</guid>
      <description>ObjectiveOne of the issues when running a multi linear problem is being able to visualize what is going on within the data graphically. Because of the limits of the human eye we are only able to see at most 3 variables and even then it’s terribly difficult. But what if there are more than 3 variables? What then? Well that’s when Principle Component Analysis comes to the rescue. Using ggbiplot a package we can use the linear combination of continuous variables and graph those components respectively after normalization.</description>
    </item>
    
    <item>
      <title>Is it an Airplane or Automobile? Neural Network using Keras and Randomly Dowloaded Images from the Internet</title>
      <link>/post/images/script/</link>
      <pubDate>Sat, 01 Dec 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/images/script/</guid>
      <description>Image ClassificationObjective: I wanted to see if I could build a Neural Network that could identify a Plane vs a Car. To do so I downloaded 12 pictures, 6 of which were airplanes and 6 that were cars. I then created the Network using Google Tensorflow and Keras Package to train the model. I then tested the model on two pictures. The results are below:
Methodology:
Download Pictures using ReadImagesNormalize Pictures since they are different sizesReshape Pictures for Model using array_reshape from Keras PackageCreate Training Set and Training Labels/Create Test Set and LabelsBuild Neural NetworkRun the ModelTest model on never before seen pictures.</description>
    </item>
    
    <item>
      <title>Neural Network on the Reuters Data Set!</title>
      <link>/post/reuters/reutersnewswire/</link>
      <pubDate>Sat, 17 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/reuters/reutersnewswire/</guid>
      <description>Objective:This is an example in a book I’m reading by Rick Scavetti. My goal is to work out this example for reinforcement and to share.
The goal of the exercise is to take 11,286 Reuters News Wires that have been bucketed into 46 possible classifications (i.e, Finance, Politics, etc.). I will be building a Neural Network on 80% of the data and then testing the network on the remaining 20%.</description>
    </item>
    
    <item>
      <title>Neural Networks - Super Cool!</title>
      <link>/post/mnist/script/</link>
      <pubDate>Fri, 16 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/mnist/script/</guid>
      <description>Objective:I am really into building models. I bought a book by Rick Scavetta. A Deep Learning Expert. I am going to blog my exercises to share and also so I can remember them.
The first exercise is building a simple Neural Network where I’ll train a model of handwritten numbers using the famous mnist dataset, which is a total of 70,000 handwritten digits from High School Students and those at the Census Bureau.</description>
    </item>
    
    <item>
      <title>Naive Bayes Model in R.</title>
      <link>/post/naivebayes/naivebayes/</link>
      <pubDate>Mon, 12 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/naivebayes/naivebayes/</guid>
      <description>Did you know?The naive model generalizes strongly that each attribute is distributed independently of any other attributes. And that is why it’s called Naive. In the real world that rarely is the case.
Objective:The objective here is to use Bayes Theroem and the Naive Bayes model to predict wither a student is admitted based on gre scores and gpa (not sure what school the data is from).</description>
    </item>
    
    <item>
      <title>Google -- Thank you for giving us Tensorflow and Keras...</title>
      <link>/post/keras_ctg/script/</link>
      <pubDate>Sat, 10 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/keras_ctg/script/</guid>
      <description>First what is Deep Learning?Deep Learning. Uhhh… Hmmmm… Well… I know how to implement some of these deep(er) learning algorithms/models, but I’m just now getting my brain around how they work. So I’m coming clean (and I feel so much lighter). My thinking is if I test my model on new data (data the model hasn’t seen) many times over and get good results (that are measurable and repeatable) no need to be concerned with how the thing works under the hood?</description>
    </item>
    
    <item>
      <title>Complete Review of the Random Forest Package</title>
      <link>/post/randomforest/rfscript/</link>
      <pubDate>Mon, 05 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/randomforest/rfscript/</guid>
      <description>Random Forest Package ReviewObjective:I wanted to learn about the Random Forest package. See what its strengths and weaknesses are. By doing this case study (inspired by Bharatendra Rai) I really have a much better understanding of the package. Moreover by writing it out and I am reinforcing my learning and helping out newcomers to the world of R and machine learning. I hope you enjoy.
Read in DataCTG Data:</description>
    </item>
    
    <item>
      <title>What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along.</title>
      <link>/post/glmnet/glmnet/</link>
      <pubDate>Mon, 05 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/glmnet/glmnet/</guid>
      <description>Objective:So in this blog I am going to go through a lesson I recently took with a gentlemen by the name Bharatendra Rai, where he goes through the glmnet model in detail.
The reason this model is great is that you can hypretune it, which you will see momentarily. It is an equal to the Random Forests but where glmnet takes the lead is with the amount of information you can pull from the model and its parameters.</description>
    </item>
    
    <item>
      <title>How many sunny days are followed by cloudy days in Seattle on any given year?</title>
      <link>/post/weather_post/mkdown/</link>
      <pubDate>Sun, 28 Oct 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/weather_post/mkdown/</guid>
      <description>Seattle Sunny Day StreaksObjective: If you’re like me when the sun comes out after a couple cloudy days my mood lifts quite a bit - even more so than when it’s been sunny for a couple days. Perhaps its the contrast between the two? Nevertheless I was curious how many shifts on average we will have from cloudy days to sunny days, in other words how many mood lifts can we estimate to have in a year living in Seattle.</description>
    </item>
    
  </channel>
</rss>