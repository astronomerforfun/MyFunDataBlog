<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Caret on Chris&#39;s Data Blog</title>
    <link>/tags/caret/</link>
    <description>Recent content in Caret on Chris&#39;s Data Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 06 Apr 2019 10:58:08 -0400</lastBuildDate>
    
	<atom:link href="/tags/caret/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Machine Learning:  Building a Predictive Model on the Cut of a Diamond.</title>
      <link>/post/diamonds_df/diamonds_cut/</link>
      <pubDate>Sat, 06 Apr 2019 10:58:08 -0400</pubDate>
      
      <guid>/post/diamonds_df/diamonds_cut/</guid>
      <description>Objective:To get some practice with the Caret Package and some of its Machine Learning Alogorithms. We will look at the famous diamonds dataset, strip out the Good and Ideal cut diamonds and then build a model that will predict future Good and Ideal cut diamonds.
Order of Operations:
Take a random sample (5,000) of the diamonds data frame using the sample_n function in dplyr.Filter the Ideal and Good diamonds using filter from dplyr.</description>
    </item>
    
    <item>
      <title>Predicting Gas Guzzlers using the Neuralnet R Package</title>
      <link>/post/neuralnet/</link>
      <pubDate>Sat, 06 Apr 2019 10:58:08 -0400</pubDate>
      
      <guid>/post/neuralnet/</guid>
      <description>Objective:To predict weather a car in the mtcars dataset is a gas guzzler or not.
Look at the distribution of mpg in the dataset by drawing a quick histogram.
Create a binary variable (1 or 0) which we will use as our Output (similar to an independent variable).Split dataset into a Training and Test set.Train Neural Network on training set.Test the Neural Network on the test set.</description>
    </item>
    
    <item>
      <title>Prevent MultiCollinearity &#43; Visualize more than 3 variables at one time.</title>
      <link>/post/pci/pci_analysis/</link>
      <pubDate>Fri, 21 Dec 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/pci/pci_analysis/</guid>
      <description>ObjectiveOne of the issues when running a multi linear problem is being able to visualize what is going on within the data graphically. Because of the limits of the human eye we are only able to see at most 3 variables and even then it’s terribly difficult. But what if there are more than 3 variables? What then? Well that’s when Principle Component Analysis comes to the rescue. Using ggbiplot a package we can use the linear combination of continuous variables and graph those components respectively after normalization.</description>
    </item>
    
    <item>
      <title>Is it an Airplane or Automobile? Neural Network using Keras and Randomly Dowloaded Images from the Internet</title>
      <link>/post/images/script/</link>
      <pubDate>Sat, 01 Dec 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/images/script/</guid>
      <description>Image ClassificationObjective: I wanted to see if I could build a Neural Network that could identify a Plane vs a Car. To do so I downloaded 12 pictures, 6 of which were airplanes and 6 that were cars. I then created the Network using Google Tensorflow and Keras Package to train the model. I then tested the model on two pictures. The results are below:
Methodology:
Download Pictures using ReadImagesNormalize Pictures since they are different sizesReshape Pictures for Model using array_reshape from Keras PackageCreate Training Set and Training Labels/Create Test Set and LabelsBuild Neural NetworkRun the ModelTest model on never before seen pictures.</description>
    </item>
    
    <item>
      <title>Neural Network on the Reuters Data Set!</title>
      <link>/post/reuters/reutersnewswire/</link>
      <pubDate>Sat, 17 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/reuters/reutersnewswire/</guid>
      <description>Objective:This is an example in a book I’m reading by Rick Scavetti. My goal is to work out this example for reinforcement and to share.
The goal of the exercise is to take 11,286 Reuters News Wires that have been bucketed into 46 possible classifications (i.e, Finance, Politics, etc.). I will be building a Neural Network on 80% of the data and then testing the network on the remaining 20%.</description>
    </item>
    
    <item>
      <title>Neural Networks - Super Cool!</title>
      <link>/post/mnist/script/</link>
      <pubDate>Fri, 16 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/mnist/script/</guid>
      <description>Objective:I am really into building models. I bought a book by Rick Scavetta. A Deep Learning Expert. I am going to blog my exercises to share and also so I can remember them.
The first exercise is building a simple Neural Network where I’ll train a model of handwritten numbers using the famous mnist dataset, which is a total of 70,000 handwritten digits from High School Students and those at the Census Bureau.</description>
    </item>
    
    <item>
      <title>Naive Bayes Model in R.</title>
      <link>/post/naivebayes/naivebayes/</link>
      <pubDate>Mon, 12 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/naivebayes/naivebayes/</guid>
      <description>Did you know?The naive model generalizes strongly that each attribute is distributed independently of any other attributes. And that is why it’s called Naive. In the real world that rarely is the case.
Objective:The objective here is to use Bayes Theroem and the Naive Bayes model to predict wither a student is admitted based on gre scores and gpa (not sure what school the data is from).</description>
    </item>
    
    <item>
      <title>Easy way to compare multiple models</title>
      <link>/post/caretcompare/caret_compare/</link>
      <pubDate>Sun, 11 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/caretcompare/caret_compare/</guid>
      <description>Objective:Get practice running models and comparing them using the caret package. Turns out that it’s very easy to run multiple algorithms in caret, compile them and compare which is the best. This solves my problem of having messy and long code. I understand too that there is a caretEnsemble package. I will not be using it in this post.
DatasetThis is a dataset that consists of 768 obs and 9 variables.</description>
    </item>
    
    <item>
      <title>Google -- Thank you for giving us Tensorflow and Keras...</title>
      <link>/post/keras_ctg/script/</link>
      <pubDate>Sat, 10 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/keras_ctg/script/</guid>
      <description>First what is Deep Learning?Deep Learning. Uhhh… Hmmmm… Well… I know how to implement some of these deep(er) learning algorithms/models, but I’m just now getting my brain around how they work. So I’m coming clean (and I feel so much lighter). My thinking is if I test my model on new data (data the model hasn’t seen) many times over and get good results (that are measurable and repeatable) no need to be concerned with how the thing works under the hood?</description>
    </item>
    
    <item>
      <title>Complete Review of the Random Forest Package</title>
      <link>/post/randomforest/rfscript/</link>
      <pubDate>Mon, 05 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/randomforest/rfscript/</guid>
      <description>Random Forest Package ReviewObjective:I wanted to learn about the Random Forest package. See what its strengths and weaknesses are. By doing this case study (inspired by Bharatendra Rai) I really have a much better understanding of the package. Moreover by writing it out and I am reinforcing my learning and helping out newcomers to the world of R and machine learning. I hope you enjoy.
Read in DataCTG Data:</description>
    </item>
    
    <item>
      <title>What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along.</title>
      <link>/post/glmnet/glmnet/</link>
      <pubDate>Mon, 05 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/glmnet/glmnet/</guid>
      <description>Objective:So in this blog I am going to go through a lesson I recently took with a gentlemen by the name Bharatendra Rai, where he goes through the glmnet model in detail.
The reason this model is great is that you can hypretune it, which you will see momentarily. It is an equal to the Random Forests but where glmnet takes the lead is with the amount of information you can pull from the model and its parameters.</description>
    </item>
    
  </channel>
</rss>