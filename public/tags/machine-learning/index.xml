<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Chris&#39;s Data Blog</title>
    <link>/tags/machine-learning/</link>
    <description>Recent content in Machine Learning on Chris&#39;s Data Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 11 Nov 2018 10:58:08 -0400</lastBuildDate>
    
	<atom:link href="/tags/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Easy way to compare multiple models</title>
      <link>/post/caretcompare/caret_compare/</link>
      <pubDate>Sun, 11 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/caretcompare/caret_compare/</guid>
      <description>Objective:Get practice running models and comparing them using the caret package. Turns out that it’s very easy to run multiple algorithms in caret, compile them and compare which is the best. This solves my problem of having messy and long code. I understand too that there is a caretEnsemble package. I will not be using it in this post.
DatasetThis is a dataset that consists of 768 obs and 9 variables.</description>
    </item>
    
    <item>
      <title>Google -- Thank you for giving us Tensorflow and Keras...</title>
      <link>/post/keras_ctg/script/</link>
      <pubDate>Sat, 10 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/keras_ctg/script/</guid>
      <description>First what is Deep Learning?Deep Learning. Uhhh… Hmmmm… Well… I know how to implement some of these deep(er) learning algorithms/models, but I’m just now getting my brain around how they work. So I’m coming clean (and I feel so much lighter). My thinking is if I test my model on new data (data the model hasn’t seen) many times over and get good results (that are measurable and repeatable) no need to be concerned with how the thing works under the hood?</description>
    </item>
    
    <item>
      <title>The ROC Curve - Is it a Mine or a Rock? - Pick your Sensitivity</title>
      <link>/post/roc/roc/</link>
      <pubDate>Tue, 06 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/roc/roc/</guid>
      <description>The Famous ROC CurveROC stands for Receiver Operating Characteristic. Its origin is from sonar back in the 40’s, where it was used to identify submarines. The ROC curve was an important metric in WWII and continues to be today.
Today the ROC curve is used in predictive modelling to distinguish between true positives and true negatives. But if you’re like me you need to see it in action to really understand.</description>
    </item>
    
    <item>
      <title>Complete Review of the Random Forest Package</title>
      <link>/post/randomforest/rfscript/</link>
      <pubDate>Mon, 05 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/randomforest/rfscript/</guid>
      <description>Random Forest Package ReviewObjective:I wanted to learn about the Random Forest package. See what its strengths and weaknesses are. By doing this case study (inspired by Bharatendra Rai) I really have a much better understanding of the package. Moreover by writing it out and I am reinforcing my learning and helping out newcomers to the world of R and machine learning. I hope you enjoy.
Read in DataCTG Data:</description>
    </item>
    
    <item>
      <title>What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along.</title>
      <link>/post/glmnet/glmnet/</link>
      <pubDate>Mon, 05 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/glmnet/glmnet/</guid>
      <description>Objective:So in this blog I am going to go through a lesson I recently took with a gentlemen by the name Bharatendra Rai, where he goes through the glmnet model in detail.
The reason this model is great is that you can hypretune it, which you will see momentarily. It is an equal to the Random Forests but where glmnet takes the lead is with the amount of information you can pull from the model and its parameters.</description>
    </item>
    
    <item>
      <title>Can the k means algorithm correctly identify a flower by its petal length and width alone?</title>
      <link>/post/kmeans/kmeans/</link>
      <pubDate>Sun, 28 Oct 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/kmeans/kmeans/</guid>
      <description>K Means ClusteringK Means Clustering is awesome. It’s what’s called unsupervised learning where there is no prediction variable. The algorithm attemps to cluster data based on its similarity. So in this case I’m going to feed the algorithm the petal width and petal length of three sets of flowers and see whether or not can appropriately group the data.
To implement k means we have to specify the number of clusters we want the data to be grouped into.</description>
    </item>
    
  </channel>
</rss>