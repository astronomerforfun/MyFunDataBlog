<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Chris&#39;s Data Blog  | What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along.</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.49.2" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    
    
      <link href="/dist/css/app.e08a958ae3e530145318b6373195c765.css" rel="stylesheet">
    

    

    
      
    

    

    <meta property="og:title" content="What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along." />
<meta property="og:description" content="Glmnet Compared to Trusty &#39;lm&#39; Model by Chris Shockley" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/glmnet/glmnet/" /><meta property="article:published_time" content="2018-11-05T10:58:08-04:00"/>
<meta property="article:modified_time" content="2018-11-05T10:58:08-04:00"/>

<meta itemprop="name" content="What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along.">
<meta itemprop="description" content="Glmnet Compared to Trusty &#39;lm&#39; Model by Chris Shockley">


<meta itemprop="datePublished" content="2018-11-05T10:58:08-04:00" />
<meta itemprop="dateModified" content="2018-11-05T10:58:08-04:00" />
<meta itemprop="wordCount" content="1713">



<meta itemprop="keywords" content="supervised learning,caret,machine learning,models,boston,tutorial," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along."/>
<meta name="twitter:description" content="Glmnet Compared to Trusty &#39;lm&#39; Model by Chris Shockley"/>

  </head>

  <body class="ma0 avenir bg-near-white">

    
   
  

  
  
  <header class="cover bg-top" style="background-image: url('/images/lasso.jpeg');">
    <div class="pb3-m pb6-l bg-black-60">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="/" class="f3 fw2 hover-white no-underline white-90 dib">
      Chris&#39;s Data Blog
    </a>
    <div class="flex-l items-center">
      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/about/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/contact/" title="Contact page">
              Contact
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white no-underline white-90" href="/post/" title="Posts page">
              Posts
            </a>
          </li>
          
        </ul>
      
      


  <a href="fb.me/astronomerforfun" class="link-transition facebook link dib z-999 pt3 pt0-l mr2" title="Facebook link">
    <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

  </a>








    </div>
  </div>
</nav>

      <div class="tc-l pv6 ph3 ph4-ns">
        <h1 class="f2 f1-l fw2 white-90 mb0 lh-title">What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along.</h1>
        
          <h2 class="fw1 f5 f3-l white-80 measure-wide-l center lh-copy mt3 mb4">
            Glmnet Compared to Trusty &#39;lm&#39; Model by Chris Shockley
          </h2>
        
      </div>
    </div>
  </header>



    <main class="pb7" role="main">
      
  <article class="flex-l flex-wrap justify-between mw8 center ph3 ph0-l">

    <header class="mt4 w-100">
      <p class="f6 b helvetica tracked">
          
        POSTS
      </p>
      <h1 class="f1 athelas mb1">What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along.</h1>
      
      <time class="f6 mv4 dib tracked" datetime="2018-11-05T10:58:08-04:00">November 5, 2018</time>
    </header>

    <main class="nested-copy-line-height lh-copy serif f4 nested-links nested-img mid-gray pr4-l w-two-thirds-l"><p><br></p>
<div id="objective" class="section level2">
<h2>Objective:</h2>
<p>So in this blog I am going to go through a lesson I recently took with a gentlemen by the name Bharatendra Rai, where he goes through the glmnet model in detail.</p>
<p>The reason this model is great is that you can hypretune it, which you will see momentarily. It is an equal to the Random Forests but where glmnet takes the lead is with the amount of information you can pull from the model and its parameters. You can also see where and which features are overfitting, which we all know is an important aspect to a healthy model.</p>
<div id="data-set" class="section level4">
<h4>Data-Set</h4>
<p>I’m going to use the Boston Housing Data Set. It consists of 506 observations and 14 variables. We are going to use the median value as our <em>Response</em> variable.</p>
<pre class="r"><code>data(&quot;BostonHousing&quot;)
df &lt;- BostonHousing
str(df)</code></pre>
<pre><code>## &#39;data.frame&#39;:    506 obs. of  14 variables:
##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
##  $ chas   : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
##  $ rad    : num  1 2 2 3 3 3 5 5 5 5 ...
##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
##  $ b      : num  397 397 393 395 397 ...
##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...</code></pre>
<p><br></p>
</div>
<div id="correlation-panel" class="section level4">
<h4>Correlation Panel</h4>
<p>When model building its important to try to prevent collinearity, which causes overfitting. In the plot below you can see all the variables and how they relate to each other. Preferrably you don’t want to see variables with high correlations, those near 1.</p>
<pre class="r"><code>pairs.panels(df[c(-4, -14)], cex = 2)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-3-1.png" width="672" /> <br></p>
</div>
</div>
<div id="split-data-into-training-and-test-sets" class="section level2">
<h2>Split Data into Training and Test Sets</h2>
<p>I will create a 70/30 Train/Test split.</p>
<pre class="r"><code>set.seed(222)
rows &lt;- sample(nrow(df), nrow(df) *.7, replace = T)
train &lt;- df[rows,]
test &lt;- df[-rows,]</code></pre>
</div>
<div id="carat-package" class="section level2">
<h2>Carat Package</h2>
<p>One of the great things about the carat package is that you can create your own custom control. Here I will use a repeated cross validation call with a 10 split, repeated 5 times.</p>
<p>Essentially, it will break the training set into 10 training and test sets and calculate the RMSE on each. It will do this 5 times. It will then average the RMSE over the 50 iterations. This gives us a good estimate of the RMSE. And a great idea by Max Kuhn (creator of package).</p>
<pre class="r"><code>custom &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10,
                       repeats = 5,
                       verboseIter = F)</code></pre>
<p><br></p>
</div>
<div id="run-the-lm-model." class="section level2">
<h2>Run the lm Model.</h2>
<p>Here I will run the old standby lm model. Below you will see that the RMSE is 4.30.</p>
<pre class="r"><code>set.seed(1234)
lm &lt;- train(medv ~ ., 
            train,
            method = &quot;lm&quot;,
            trControl = custom)</code></pre>
<pre class="r"><code>options(scipen = 9999)

lm</code></pre>
<pre><code>## Linear Regression 
## 
## 354 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 319, 318, 319, 318, 319, 319, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   4.303338  0.7678331  3.185276
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p><br></p>
</div>
<div id="glmnet-model-ridge-regression" class="section level2">
<h2>Glmnet Model (Ridge Regression)</h2>
<p><em>Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value.</em></p>
<p>On this one I will use the glmnet model where I will set the alpha to 0 and the lambda with small values up to 1. I will run it through the training set and then take a look.</p>
<p>I can see that the RMSE for this model is slightly higher than the lm model with an RMSE of 4.39 with 76% of the variance being explained by the features/variables.</p>
<p><br></p>
</div>
<div id="interpretation-of-graphs" class="section level2">
<h2>Interpretation of Graphs</h2>
<p>I can see that the lambda of .05 gives the best result. In this model.</p>
<pre class="r"><code>plot(ridge)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-9-1.png" width="672" /> <br></p>
<p>We can see in this plot that as lambda decreases the coefficients start to increase resulting in higher Squared Errors. Lambda also adds a small penalty to those features that don’t contribute much to the model. So from top you can see that all 13 vaiables are in play throughout. From right to left you can see some of the variables increasing in size, however the blue one is growing at a greater rate and negative. In other words, variables 4 and 6 (you can see the small numbers at the far left) will have a bigger influence on the model.</p>
<pre class="r"><code>plot(ridge$finalModel, xvar = &quot;lambda&quot;, label = T)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-10-1.png" width="672" /> <br> In this plot you can see that from left to right that at .2 of the variability is being explained with a slight growth in the coefficients. And at .7 or 70% of the variance is explained. But the coeffients are becoming much larger. By the time it gets to 80 the coeffients grow way to large and is likely the result of overfitting.</p>
<pre class="r"><code>plot(ridge$finalModel, xvar = &quot;dev&quot;, label = T)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-11-1.png" width="672" /> <br></p>
</div>
<div id="variable-importance" class="section level2">
<h2>Variable Importance</h2>
<p>This is great because this function allows me to see what variables are the most important in the model. If not too important I could take them out all together or apply a penalty to them to reduce them to 0.</p>
<pre class="r"><code>plot(varImp(ridge, scale = T))</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="lasso-regression" class="section level2">
<h2>Lasso Regression</h2>
<p><em>Lasso method. The only difference from Ridge regression is that the regularization term is in absolute value. … Lasso method overcomes the disadvantage of Ridge regression by not only punishing high values of the coefficients but actually setting them to zero if they are not relevant.</em></p>
<p>In this example I’ll use an alpha of 1 fixed and a lambda between .0001 and .2.</p>
<p>The RMSE is now 4.3 and the optimal lambda is .0001.</p>
<pre class="r"><code>set.seed(1234)

lasso &lt;- train(medv ~ ., 
               train,
               method = &quot;glmnet&quot;,
               tuneGrid = expand.grid(alpha = 1,
                                      lambda = seq(.0001, .2, length = 5)),
               trControl = custom)

lasso</code></pre>
<pre><code>## glmnet 
## 
## 354 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 319, 318, 319, 318, 319, 319, ... 
## Resampling results across tuning parameters:
## 
##   lambda    RMSE      Rsquared   MAE     
##   0.000100  4.303214  0.7677303  3.175517
##   0.050075  4.317879  0.7654724  3.134688
##   0.100050  4.375287  0.7585802  3.142376
##   0.150025  4.466113  0.7482112  3.191947
##   0.200000  4.507634  0.7437641  3.206663
## 
## Tuning parameter &#39;alpha&#39; was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.0001.</code></pre>
<p><br></p>
</div>
<div id="explanation-of-graphs" class="section level2">
<h2>Explanation of Graphs</h2>
<p>I can see that a very small alpha is optimal. That is consistent with the optimal value of lamda the model stated (.0001).</p>
<pre class="r"><code>plot(lasso)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p><br></p>
<p>I can see that asa lamda decreases the variables increase slowly. The next chart shows what I’m looking for though.</p>
<pre class="r"><code>plot(lasso$finalModel, xvar = &quot;lambda&quot;, label = T)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-15-1.png" width="672" /> <br></p>
<p>This plot shows that 60% of the variance is explained by only 3 variables. So there are some duds features in the model or collinearity.</p>
<pre class="r"><code>plot(lasso$finalModel, xvar = &quot;dev&quot;, label = T)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p><br></p>
<p>How about Variable Importance?</p>
<pre class="r"><code>plot(varImp(lasso, scale = T))</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-17-1.png" width="672" /> <br></p>
</div>
<div id="now-how-about-a-combined-model-where-we-do-ridge-and-lasso-regression-otherwise-known-as-net-regression-hence-the-name-of-the-package-glmnet" class="section level2">
<h2>Now how about a combined model where we do ridge and lasso regression otherwise known as Net Regression, hence the name of the package “glmnet”?</h2>
<p>Now we have an RMSE of 4.3 an optimal alpha of .1111 and lamda of .0001. Best model yet. Now I’m going to skip the graphs this time as they are relatively the same. But I am going to compile the models for comparison.</p>
<p><br></p>
</div>
<div id="comparison" class="section level2">
<h2>Comparison</h2>
<p>Turns out that Lasso Regression had the best RMSE and the Net model had the best R Squared (Look at the Median Column). All of them were very close. I was surprised at how well the lm model held its own.</p>
<pre class="r"><code>model_list &lt;- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res &lt;- resamples(model_list)
summary(res)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = res)
## 
## Models: LinearModel, Ridge, Lasso, ElasticNet 
## Number of resamples: 50 
## 
## MAE 
##                 Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## LinearModel 2.352321 2.899379 3.063309 3.185276 3.506306 4.062280    0
## Ridge       2.292097 2.769948 3.016595 3.076879 3.324034 4.135409    0
## Lasso       2.347178 2.895592 3.059505 3.175517 3.492517 4.066730    0
## ElasticNet  2.348923 2.892819 3.058201 3.174246 3.492608 4.067000    0
## 
## RMSE 
##                 Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## LinearModel 3.218283 3.683662 4.097799 4.303338 4.777701 6.503843    0
## Ridge       3.087577 3.652257 4.168664 4.351969 4.768355 7.076786    0
## Lasso       3.211613 3.674361 4.094753 4.303214 4.794841 6.539509    0
## ElasticNet  3.208691 3.674259 4.096475 4.302170 4.787268 6.539673    0
## 
## Rsquared 
##                  Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
## LinearModel 0.5389469 0.7174655 0.7830427 0.7678331 0.8221920 0.8896646
## Ridge       0.4590974 0.7049716 0.7888941 0.7624169 0.8248683 0.8992497
## Lasso       0.5360178 0.7158311 0.7838163 0.7677303 0.8220441 0.8903956
## ElasticNet  0.5355595 0.7165078 0.7836337 0.7678382 0.8222248 0.8904117
##             NA&#39;s
## LinearModel    0
## Ridge          0
## Lasso          0
## ElasticNet     0</code></pre>
<p><br></p>
<p>Box plot of the models. It’s hard to tell though which is best since they are so close. The table above gives a more precise metric.</p>
<pre class="r"><code>bwplot(res, metric = &quot;RMSE&quot;)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p><br></p>
</div>
<div id="future-analysis" class="section level2">
<h2>Future Analysis</h2>
<p>The only thing left to do is to predict on the test set using the model. I will do that in the future. I suspect that the RMSE will be slightly higher than the models but not much.</p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion:</h2>
<p>I learned a great deal through his course and teaching it back to myself and you. I now have a greater understanding of how to tune models. I will practice more in future blogs. Glmnet is my goto when I need more information then a Random Forest can offer.</p>
</div>
<ul class="pa0">
  
   <li class="list">
     <a href="/tags/supervised-learning" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">supervised learning</a>
   </li>
  
   <li class="list">
     <a href="/tags/caret" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">caret</a>
   </li>
  
   <li class="list">
     <a href="/tags/machine-learning" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">machine learning</a>
   </li>
  
   <li class="list">
     <a href="/tags/models" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">models</a>
   </li>
  
   <li class="list">
     <a href="/tags/boston" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">boston</a>
   </li>
  
   <li class="list">
     <a href="/tags/tutorial" class="link f5 grow no-underline br-pill ba ph3 pv2 mb2 dib black sans-serif">tutorial</a>
   </li>
  
</ul>
<div class="mt6">
        <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "www-shockleysblog-com" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </div>
    </main>

    <aside class="w-30-l mt6-l">




  <div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links">
    <p class="f5 b mb3">Related</p>
    <ul class="pa0 list">
	   
	     <li  class="mb2">
          <a href="/post/randomforest/rfscript/">Complete Review of the Random Forest Package</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/showdown/showdown/">Showdown:  Random Forest vs Regression on EPA MPG Dataset</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/weather_post/mkdown/">How many sunny days are followed by cloudy days in Seattle on any given year?</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/kmeans/kmeans/">Can the k means algorithm correctly identify a flower by its petal length and width alone?</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/website/rvest_tables_tutorial/">Tutorial on Web Scraping (Basic) - Wiki Tables</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/regex/untitled/">To Regex or to Not?</a>
        </li>
	    
	     <li  class="mb2">
          <a href="/post/melt/melt/">Why Melt and Cast When You Can Recast?</a>
        </li>
	    
    </ul>
</div>

</aside>

  </article>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="/" >
    &copy; 2018 Chris&#39;s Data Blog
  </a>
    <div>


  <a href="fb.me/astronomerforfun" class="link-transition facebook link dib z-999 pt3 pt0-l mr2" title="Facebook link">
    <svg height="32px"  style="enable-background:new 0 0 67 67;" version="1.1" viewBox="0 0 67 67" width="32px" xml:space="preserve" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><path d="M28.765,50.32h6.744V33.998h4.499l0.596-5.624h-5.095  l0.007-2.816c0-1.466,0.14-2.253,2.244-2.253h2.812V17.68h-4.5c-5.405,0-7.307,2.729-7.307,7.317v3.377h-3.369v5.625h3.369V50.32z   M33,64C16.432,64,3,50.569,3,34S16.432,4,33,4s30,13.431,30,30S49.568,64,33,64z" style="fill-rule:evenodd;clip-rule:evenodd;"/></svg>

  </a>







</div>
  </div>
</footer>

    

  <script src="/dist/js/app.3fc0f988d21662902933.js"></script>


  </body>
</html>
