<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>My Fun Data Analytics Blog on Chris&#39;s Data Blog</title>
    <link>/</link>
    <description>Recent content in My Fun Data Analytics Blog on Chris&#39;s Data Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 14 Apr 2019 10:58:08 -0400</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Multiple Model Comparison on Boston Housing Data</title>
      <link>/post/ageofhouse/</link>
      <pubDate>Sun, 14 Apr 2019 10:58:08 -0400</pubDate>
      
      <guid>/post/ageofhouse/</guid>
      <description>Comparing Multiple Models:Objective:
The objective of this script is to predict the age of a home in the Boston Data Set for the purpose of comparing multiple algorithms.
Since the purpose is to compare models I’ll be spending less time looking at the actual model results.
Data Set:
The data set is comprised of 506 rows and 14 columns. The variables range from crime per capita by town to the pupil-teacher ratio by town.</description>
    </item>
    
    <item>
      <title>Machine Learning:  Building a Predictive Model on the Cut of a Diamond.</title>
      <link>/post/diamonds_df/diamonds_cut/</link>
      <pubDate>Sat, 06 Apr 2019 10:58:08 -0400</pubDate>
      
      <guid>/post/diamonds_df/diamonds_cut/</guid>
      <description>Objective:To get some practice with the Caret Package and some of its Machine Learning Alogorithms. We will look at the famous diamonds dataset, strip out the Good and Ideal cut diamonds and then build a model that will predict future Good and Ideal cut diamonds.
Order of Operations:
Take a random sample (5,000) of the diamonds data frame using the sample_n function in dplyr.Filter the Ideal and Good diamonds using filter from dplyr.</description>
    </item>
    
    <item>
      <title>Predicting Gas Guzzlers using the Neuralnet R Package</title>
      <link>/post/neuralnet/</link>
      <pubDate>Sat, 06 Apr 2019 10:58:08 -0400</pubDate>
      
      <guid>/post/neuralnet/</guid>
      <description>Objective:To predict weather a car in the mtcars dataset is a gas guzzler or not.
Look at the distribution of mpg in the dataset by drawing a quick histogram.
Create a binary variable (1 or 0) which we will use as our Output (similar to an independent variable).Split dataset into a Training and Test set.Train Neural Network on training set.Test the Neural Network on the test set.</description>
    </item>
    
    <item>
      <title>Prevent MultiCollinearity &#43; Visualize more than 3 variables at one time.</title>
      <link>/post/pci/pci_analysis/</link>
      <pubDate>Fri, 21 Dec 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/pci/pci_analysis/</guid>
      <description>ObjectiveOne of the issues when running a multi linear problem is being able to visualize what is going on within the data graphically. Because of the limits of the human eye we are only able to see at most 3 variables and even then it’s terribly difficult. But what if there are more than 3 variables? What then? Well that’s when Principle Component Analysis comes to the rescue. Using ggbiplot a package we can use the linear combination of continuous variables and graph those components respectively after normalization.</description>
    </item>
    
    <item>
      <title>Is it an Airplane or Automobile? Neural Network using Keras and Randomly Dowloaded Images from the Internet</title>
      <link>/post/images/script/</link>
      <pubDate>Sat, 01 Dec 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/images/script/</guid>
      <description>Image ClassificationObjective: I wanted to see if I could build a Neural Network that could identify a Plane vs a Car. To do so I downloaded 12 pictures, 6 of which were airplanes and 6 that were cars. I then created the Network using Google Tensorflow and Keras Package to train the model. I then tested the model on two pictures. The results are below:
Methodology:
Download Pictures using ReadImagesNormalize Pictures since they are different sizesReshape Pictures for Model using array_reshape from Keras PackageCreate Training Set and Training Labels/Create Test Set and LabelsBuild Neural NetworkRun the ModelTest model on never before seen pictures.</description>
    </item>
    
    <item>
      <title>If you&#39;re considering learning R start with Dplyr and work your way from there!</title>
      <link>/post/dplyr/test/</link>
      <pubDate>Thu, 22 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/dplyr/test/</guid>
      <description>Objective:I wanted to give a quick once over of dplyr in less than 10 minutes. This is an intro that will get you started. After watching the video pull down the Cheat sheet off of R Studio’s Website and study it. Practice for two weeks and if you like what you see give it another week and then move full time into R for analytics (you can pick up what you’re missing while you’re moving forwards).</description>
    </item>
    
    <item>
      <title>Neural Network on the Reuters Data Set!</title>
      <link>/post/reuters/reutersnewswire/</link>
      <pubDate>Sat, 17 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/reuters/reutersnewswire/</guid>
      <description>Objective:This is an example in a book I’m reading by Rick Scavetti. My goal is to work out this example for reinforcement and to share.
The goal of the exercise is to take 11,286 Reuters News Wires that have been bucketed into 46 possible classifications (i.e, Finance, Politics, etc.). I will be building a Neural Network on 80% of the data and then testing the network on the remaining 20%.</description>
    </item>
    
    <item>
      <title>Neural Networks - Super Cool!</title>
      <link>/post/mnist/script/</link>
      <pubDate>Fri, 16 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/mnist/script/</guid>
      <description>Objective:I am really into building models. I bought a book by Rick Scavetta. A Deep Learning Expert. I am going to blog my exercises to share and also so I can remember them.
The first exercise is building a simple Neural Network where I’ll train a model of handwritten numbers using the famous mnist dataset, which is a total of 70,000 handwritten digits from High School Students and those at the Census Bureau.</description>
    </item>
    
    <item>
      <title>An Easier Way to Communicate on Projects (good bye email)</title>
      <link>/post/slack/slackblog/</link>
      <pubDate>Thu, 15 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/slack/slackblog/</guid>
      <description>What is Slack?Slack is to project communication what dropbox is to sharing documents. A formal definition is a web based software that serves as a communication hub for those invited.
These guys got it right too. I recently started a consulting gig where I was introduced to the app. I found that the CEO on down to the developers were incredibly active – in fact they didn’t use email they just used the message board to communicate.</description>
    </item>
    
    <item>
      <title>Naive Bayes Model in R.</title>
      <link>/post/naivebayes/naivebayes/</link>
      <pubDate>Mon, 12 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/naivebayes/naivebayes/</guid>
      <description>Did you know?The naive model generalizes strongly that each attribute is distributed independently of any other attributes. And that is why it’s called Naive. In the real world that rarely is the case.
Objective:The objective here is to use Bayes Theroem and the Naive Bayes model to predict wither a student is admitted based on gre scores and gpa (not sure what school the data is from).</description>
    </item>
    
    <item>
      <title>Easy way to compare multiple models</title>
      <link>/post/caretcompare/caret_compare/</link>
      <pubDate>Sun, 11 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/caretcompare/caret_compare/</guid>
      <description>Objective:Get practice running models and comparing them using the caret package. Turns out that it’s very easy to run multiple algorithms in caret, compile them and compare which is the best. This solves my problem of having messy and long code. I understand too that there is a caretEnsemble package. I will not be using it in this post.
DatasetThis is a dataset that consists of 768 obs and 9 variables.</description>
    </item>
    
    <item>
      <title>Google -- Thank you for giving us Tensorflow and Keras...</title>
      <link>/post/keras_ctg/script/</link>
      <pubDate>Sat, 10 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/keras_ctg/script/</guid>
      <description>First what is Deep Learning?Deep Learning. Uhhh… Hmmmm… Well… I know how to implement some of these deep(er) learning algorithms/models, but I’m just now getting my brain around how they work. So I’m coming clean (and I feel so much lighter). My thinking is if I test my model on new data (data the model hasn’t seen) many times over and get good results (that are measurable and repeatable) no need to be concerned with how the thing works under the hood?</description>
    </item>
    
    <item>
      <title>Lucid is so easy to use I think I&#39;m going to Process Flow cleaning my house.</title>
      <link>/post/lucid/lucid/</link>
      <pubDate>Fri, 09 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/lucid/lucid/</guid>
      <description>First Look at LucidObjective:I have been tasked with helping document processes for our company as we are begining the process of overhauling our systems/processes and incorporating an ERP system.
One of the tools I was introducted to for this project was Lucid https://www.lucidchart.com. I thought I would give my initial thoughts on the software thus far.
So what is it?Lucid is a web process mapping software. You sign up, pay a subscription fee and you can use and publish as you see fit.</description>
    </item>
    
    <item>
      <title>The ROC Curve - Is it a Mine or a Rock? - Pick your Sensitivity</title>
      <link>/post/roc/roc/</link>
      <pubDate>Tue, 06 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/roc/roc/</guid>
      <description>The Famous ROC CurveROC stands for Receiver Operating Characteristic. Its origin is from sonar back in the 40’s, where it was used to identify submarines. The ROC curve was an important metric in WWII and continues to be today.
Today the ROC curve is used in predictive modelling to distinguish between true positives and true negatives. But if you’re like me you need to see it in action to really understand.</description>
    </item>
    
    <item>
      <title>Complete Review of the Random Forest Package</title>
      <link>/post/randomforest/rfscript/</link>
      <pubDate>Mon, 05 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/randomforest/rfscript/</guid>
      <description>Random Forest Package ReviewObjective:I wanted to learn about the Random Forest package. See what its strengths and weaknesses are. By doing this case study (inspired by Bharatendra Rai) I really have a much better understanding of the package. Moreover by writing it out and I am reinforcing my learning and helping out newcomers to the world of R and machine learning. I hope you enjoy.
Read in DataCTG Data:</description>
    </item>
    
    <item>
      <title>Showdown:  Random Forest vs Regression on EPA MPG Dataset</title>
      <link>/post/showdown/showdown/</link>
      <pubDate>Mon, 05 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/showdown/showdown/</guid>
      <description>Objective:I’m always curious about which model performs the best on data frames. Today I’m going to use the mpg dataset, which contains a subset of the fuel economy data the EPA makes available on http://fueleconomy.gov.
I am going to use city miles per gallon as the response variable and the rest of the variables as predictors, or dependent variables. I am also going to use the RMSE measure to compare the models.</description>
    </item>
    
    <item>
      <title>What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along.</title>
      <link>/post/glmnet/glmnet/</link>
      <pubDate>Mon, 05 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/glmnet/glmnet/</guid>
      <description>Objective:So in this blog I am going to go through a lesson I recently took with a gentlemen by the name Bharatendra Rai, where he goes through the glmnet model in detail.
The reason this model is great is that you can hypretune it, which you will see momentarily. It is an equal to the Random Forests but where glmnet takes the lead is with the amount of information you can pull from the model and its parameters.</description>
    </item>
    
    <item>
      <title>Mushroom Model</title>
      <link>/post/mushroom/mushrooms/</link>
      <pubDate>Thu, 01 Nov 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/mushroom/mushrooms/</guid>
      <description>Another FFTrees PostThis is great. Have you ever been on a walk in a creek and came across some mushrooms? And you think to yourself, “Man those look good? Perhaps I should bring those home?” Ok maybe not.
Well this blog is about building an FFtrees model developed by Nathaniel Phillips. The dataset consists of 8,124 mushrooms that are classified as Poisonous or Non Poisonous. Additionally there are 22 different variables identifying the mushroom.</description>
    </item>
    
    <item>
      <title>Can the k means algorithm correctly identify a flower by its petal length and width alone?</title>
      <link>/post/kmeans/kmeans/</link>
      <pubDate>Sun, 28 Oct 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/kmeans/kmeans/</guid>
      <description>K Means ClusteringK Means Clustering is awesome. It’s what’s called unsupervised learning where there is no prediction variable. The algorithm attemps to cluster data based on its similarity. So in this case I’m going to feed the algorithm the petal width and petal length of three sets of flowers and see whether or not can appropriately group the data.
To implement k means we have to specify the number of clusters we want the data to be grouped into.</description>
    </item>
    
    <item>
      <title>How many sunny days are followed by cloudy days in Seattle on any given year?</title>
      <link>/post/weather_post/mkdown/</link>
      <pubDate>Sun, 28 Oct 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/weather_post/mkdown/</guid>
      <description>Seattle Sunny Day StreaksObjective: If you’re like me when the sun comes out after a couple cloudy days my mood lifts quite a bit - even more so than when it’s been sunny for a couple days. Perhaps its the contrast between the two? Nevertheless I was curious how many shifts on average we will have from cloudy days to sunny days, in other words how many mood lifts can we estimate to have in a year living in Seattle.</description>
    </item>
    
    <item>
      <title>Tutorial on Web Scraping (Basic) - Wiki Tables</title>
      <link>/post/website/rvest_tables_tutorial/</link>
      <pubDate>Wed, 10 Oct 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/website/rvest_tables_tutorial/</guid>
      <description>Web Scraping Tutorial (Wiki Tables)One of my favorite things to do is to scrape web data for analysis in R. Below you will find a simple example where we will scrape a list of Messier Objects from Wiki.
Workflow for Scraping Wiki TablesRead in page needed to scrape;Scrape the Table using Rvest (Wiki Tables do not need the Node function);Convert to Data Frame;Clean up Data;Analyze.</description>
    </item>
    
    <item>
      <title>To Regex or to Not?</title>
      <link>/post/regex/untitled/</link>
      <pubDate>Tue, 09 Oct 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/regex/untitled/</guid>
      <description>Is Regex Necessary?Yep. If you’re new to programming or R. Learn Regex. It will help you so much when your data asks of you to do data-gymnastics. Below you will find the steps I took to start learning Regex.
Online Tutorialhttp://www.regexone.com
This is a great site to get your feet wet. You can do it in an hour and when you’re done you’ll have a working knowledge of Regex and be able to do the most common Regex tasks.</description>
    </item>
    
    <item>
      <title>Summer is Leaving Town</title>
      <link>/post/summer_leaving/summer/</link>
      <pubDate>Sun, 09 Sep 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/summer_leaving/summer/</guid>
      <description>I’ve never been good with saying good-bye. But that’s exactly what I find myself doing. Waving good bye to Summer and its caravans of warm sunny days, clear lakes and long evenings trail behind. And even if the curtain comes back up in September and Summer plays a finale – I know – the show – is coming to an end. It just sucks!
Over the past few months I have had the best time – from my first overnight hike (in the dark) at Ancient Lakes to climbing 4,000 ft with my Niece at Crystal Mountain inside Mount Rainier National Park.</description>
    </item>
    
    <item>
      <title>Why Melt and Cast When You Can Recast?</title>
      <link>/post/melt/melt/</link>
      <pubDate>Mon, 09 Jul 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/melt/melt/</guid>
      <description>Reshape Data Practice (Melt/Casting/Recast)
Objective:
No this isn’t about a tuna melt, though looking at this sandwich makes me hungry. Rather this about the melt function and others in the Reshape package.
If you’re like me you have to go look up the documentation each time a reshape is needed for data. I want to get ahead of the curve and practice. I found a great website with some exercises written by John Akwei - it can be found @ https://www.</description>
    </item>
    
    <item>
      <title>Quick EA on Messier and Herschel Catalogues</title>
      <link>/post/messier_herschel/messierherschel/</link>
      <pubDate>Sat, 09 Jun 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/messier_herschel/messierherschel/</guid>
      <description>Quick EA on Messier and Herschel Data Sets
Purpose:
I have had this data set for a while. I was curious to see if there were any patterns within it. While this analysis is exploratory in nature and I didn’t set out to answer a particular question, I did, however, find myself asking questions. Like how did Messier miss all the objects that Herschel had found in Ursa Major (Big Dipper)?</description>
    </item>
    
    <item>
      <title>Analytics on Classified Section of Cloudy Nights (5,546 ads)</title>
      <link>/post/cloudy_nights/cloudy_nights_scrape/</link>
      <pubDate>Mon, 09 Apr 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/cloudy_nights/cloudy_nights_scrape/</guid>
      <description>Introduction:
I wanted to see if there were any trends within the Cloudy Nights Classifieds. I obtained the data using R. I also used R for the analysis.
If you have any questions or concerns you can reach out to me via shockley7772@icloud.com
Histogram of the number of Views per Ad.
ggplot(data, aes(Views_final)) + geom_bar(col = &amp;quot;steelblue&amp;quot;) + labs(title = &amp;quot;Histogram of Views per Classified ad&amp;quot;, subtitle = &amp;quot;Eyepiece Classifieds Only&amp;quot;, x= &amp;quot;Number of Views per Ad&amp;quot;, y = &amp;quot;Frequency&amp;quot;, caption = &amp;quot;Source: Cloudy Nights Classifieds&amp;quot;) + theme_bw() Looks as if most of the classified ads get around 250 looks.</description>
    </item>
    
    <item>
      <title>Easter Sermon Sentiment Analysis</title>
      <link>/post/easter_sermon/tim/</link>
      <pubDate>Mon, 09 Apr 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/easter_sermon/tim/</guid>
      <description>Objective:I was curious what a sermon would look like doing text analysis with the Syuzhet Package (https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html). I chose Pastor Tim Knight’s Easter message. I obatined the mp4 off of the Church website and converted it to text using an application (I didn’t know that could be done). I then uploaded the text into R and began the analysis which can be seen below.</description>
    </item>
    
    <item>
      <title>Eno Double Nest Hammock Review Analysis (Amazon User Reviews)</title>
      <link>/post/eno_hammock/test/</link>
      <pubDate>Mon, 09 Apr 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/eno_hammock/test/</guid>
      <description>IntroductionThis analysis was performed as an experiment to see what can be derived from User Reviews. The questions were as follows:
What is the distribution of Reviews as it relates to star rankings?How do the Number of Reviews compare over time?How do the Number of Reviews compare over time by star ranking and count?What is the average sentiment score using SentimentR package?</description>
    </item>
    
    <item>
      <title>Jim Comey&#39;s A Higher Loyalty Amazon Review Analysis</title>
      <link>/post/jim_comey/comey_markdown/</link>
      <pubDate>Mon, 09 Apr 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/jim_comey/comey_markdown/</guid>
      <description>Findings:It is estimated that 6% of purchasers on Amazon write reviews. If that number is constant than there was a sharp decline in Sales shortly after the publication of Mr. Comey’s Book.
I also found that the Kindle Edition of the book outsold (based on the reviews) Hardcover. It makes sense since Kindle Books are cheaper and faster to receive.
The Bigram Word Cloud turned out well and is informative.</description>
    </item>
    
    <item>
      <title>Red Wine Analysis</title>
      <link>/post/red_wine/wine_report/</link>
      <pubDate>Mon, 09 Apr 2018 10:58:08 -0400</pubDate>
      
      <guid>/post/red_wine/wine_report/</guid>
      <description>Objective: My objective was to see if I could predict whether a wine was good or bad using the Random Forest Algorithm (using Random Forest and FFTrees). To run the model I train it on 70% of the data. I then test it against the test set or the other 30% to see how it does. Before I start the random Forest I do some Exploratory Analysis to see those factors that contribute to a wine being classified a Good Wine.</description>
    </item>
    
    <item>
      <title>Contact</title>
      <link>/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/contact/</guid>
      <description>My email address is chris.shockley@gmail.com. My github repository can be found at https://github.com/astronomerforfun.</description>
    </item>
    
  </channel>
</rss>