---
date: 2018-11-10T10:58:08-04:00
description: "Keras Deep Learning Neural Network by Chris Shockley"
featured_image: "/images/kerastensor.jpg"
tags: ["supervised learning", "caret", "machine learning", "models", "keras", "tutorial" , "tensorflow"]
title: "Google -- Thank you for giving us Tensorflow and Keras..."
---



<div id="first-what-is-deep-learning" class="section level2">
<h2>First what is Deep Learning?</h2>
<p>Deep Learning. What? Ok I confess. I know how to implement some of these deep(er) learning algorithms, but I’m just now getting my brain around how they work. And as caveat. I’m focused driven. My thinking is if I test my model on new data (data the model hasn’t seen) many times over and get good results, what the heck do I care how the thing works under the hood? Or to draw an analogy. If a friend lets me borrow his Porsche and I going to get under the hood? Nope. But I digress. I’m just now getting my brain around it, so you’ll have to wait for a thorough blog post on exactly what it is.</p>
<p>Moreover, Keras and Tensorflow (Google’s gift to the developers involved in Machine Learning - yes this is Google’s API, which FB and everyother tech company is using) has been limited to those that use Python. More recently R has joined the AI space so that’s cool – if you’re into R we don’t have to learn another programming language to implement these Neural Networks.</p>
<div id="objective" class="section level4">
<h4>Objective:</h4>
<p>I am going to work train a Neural Network in R within Keras. The tutorial is meant for practice for me and a document of sorts. And for you too. If you’re interested follow along. The code will be in R of course.</p>
</div>
</div>
<div id="load-packages" class="section level2">
<h2>Load Packages</h2>
<pre class="r"><code>library(keras)
install_keras()</code></pre>
<pre><code>## Creating r-tensorflow conda environment for TensorFlow installation...
## 
## Installation complete.</code></pre>
<p><br></p>
</div>
<div id="load-the-data" class="section level2">
<h2>Load the Data</h2>
<p>I am using the CTG dataset. There are 21 independent variables and one dependent variable. This will be a classifcation problem. We are predicting whether the dependent variable is a 1,2, or 3.</p>
<pre class="r"><code>data &lt;- read.csv(&quot;CTG.csv&quot;, header = T)
head(data)</code></pre>
<pre><code>##    LB          AC FM          UC          DL DS          DP ASTV MSTV ALTV
## 1 120 0.000000000  0 0.000000000 0.000000000  0 0.000000000   73  0.5   43
## 2 132 0.006379585  0 0.006379585 0.003189793  0 0.000000000   17  2.1    0
## 3 133 0.003322259  0 0.008305648 0.003322259  0 0.000000000   16  2.1    0
## 4 134 0.002560819  0 0.007682458 0.002560819  0 0.000000000   16  2.4    0
## 5 132 0.006514658  0 0.008143322 0.000000000  0 0.000000000   16  2.4    0
## 6 134 0.001049318  0 0.010493179 0.009443861  0 0.002098636   26  5.9    0
##   MLTV Width Min Max Nmax Nzeros Mode Mean Median Variance Tendency NSP
## 1  2.4    64  62 126    2      0  120  137    121       73        1   2
## 2 10.4   130  68 198    6      1  141  136    140       12        0   1
## 3 13.4   130  68 198    5      1  141  135    138       13        0   1
## 4 23.0   117  53 170   11      0  137  134    137       13        1   1
## 5 19.9   117  53 170    9      0  137  136    138       11        1   1
## 6  0.0   150  50 200    5      3   76  107    107      170        0   3</code></pre>
<p><br></p>
</div>
<div id="proper-form" class="section level2">
<h2>Proper Form</h2>
<p>To get this in the proper form for Keras I need to convert it to a Matrix and then remove the column names.</p>
<pre class="r"><code>data &lt;- as.matrix(data)
dimnames(data) &lt;- NULL</code></pre>
<p><br></p>
<div id="normalization-and-other-things" class="section level4">
<h4>Normalization and Other Things</h4>
<p>For Keras to run the algorithm properly I must normalize all the independent variables/columns, with the exception of the dependent/response variable. That I am going to convert from a factor to a numeric.</p>
<pre class="r"><code>data[, 1:21] &lt;- normalize(data[,1:21])
data[,22] &lt;- as.numeric(data[,22])-1</code></pre>
<p><br></p>
</div>
<div id="create-test-and-training-set" class="section level4">
<h4>Create Test and Training Set</h4>
<p>Here I am going to create a Training and Test Set using a 70/30 split. Additionally I will store the dependent variable in its own object.</p>
<pre class="r"><code>set.seed(1234) #for reproducibility
ind &lt;- sample(2, nrow(data), replace = T, prob = c(.7,.3)) # This is new way of splitting data
training &lt;- data[ind == 1, 1:21] #Independent
test &lt;- data[ind==2, 1:21] #Independent
trainingtarget &lt;- data[ind == 1, 22] #dependent
testtarget &lt;- data[ind == 2, 22] #dependent</code></pre>
<p><br></p>
</div>
<div id="one-hot-encoding" class="section level4">
<h4>One Hot Encoding</h4>
<p>This is one of the crazy cool features of Keras. Basically it takes the matrix and expands it out so that each row has only one “1”. This is how Keras will train the model. Look at the first ten rows to see what it did. Basically were back to binary’s (1’s and 0’s )</p>
<pre class="r"><code>trainlabels &lt;- to_categorical(trainingtarget)
testlabels &lt;- to_categorical(testtarget)
head(trainlabels, 10)</code></pre>
<pre><code>##       [,1] [,2] [,3]
##  [1,]    0    1    0
##  [2,]    1    0    0
##  [3,]    1    0    0
##  [4,]    1    0    0
##  [5,]    0    0    1
##  [6,]    0    0    1
##  [7,]    0    0    1
##  [8,]    0    0    1
##  [9,]    0    0    1
## [10,]    0    1    0</code></pre>
<p><br></p>
</div>
<div id="model" class="section level4">
<h4>Model</h4>
<p>So this is going to be a Neural Network with two hidden layers. I’m going to have to write up a seperate blog post on what this all means because it’s too hard to explain in a paragraph. But if I was to put it plainly the model is going to be pushed through two heavily dense neural networks. The more layers the denser and deeper the model. Of course the more computing power as well. I should be fine here.</p>
<p>Relu and Softmax are two of the most popular layers to use. There are others and I’ll explain that in a different blog. This is just down and dirty stuff/get er’ type blog here.</p>
<pre class="r"><code>model &lt;- keras_model_sequential()
model %&gt;%
  layer_dense(units = 8, activation = &quot;relu&quot;, input_shape = c(21))%&gt;%
  layer_dense(units = 3, activation = &quot;softmax&quot;)
summary(model)</code></pre>
<pre><code>## ___________________________________________________________________________
## Layer (type)                     Output Shape                  Param #     
## ===========================================================================
## dense_1 (Dense)                  (None, 8)                     176         
## ___________________________________________________________________________
## dense_2 (Dense)                  (None, 3)                     27          
## ===========================================================================
## Total params: 203
## Trainable params: 203
## Non-trainable params: 0
## ___________________________________________________________________________</code></pre>
<p><br></p>
</div>
</div>
<div id="learning" class="section level2">
<h2>Learning</h2>
<p>Now we’re going to set up the model itself.</p>
<pre class="r"><code>model %&gt;%
  compile(loss = &quot;categorical_crossentropy&quot;,
          optimizer = &quot;adam&quot;,
          metrics = &quot;accuracy&quot;)</code></pre>
<p><br></p>
</div>
<div id="fit-model" class="section level2">
<h2>Fit Model</h2>
<p>This is where we will fit the model. Notice that I reserved .2 as validation. So as it’s fitting the model it will be testing it against the validation portion. This will give us a good understanding of how it’s performing with the training set. See graph.</p>
<p>In the graph you can see at the top (which is Loss) the training loss is decreasing but the validation starts to increase. That’s because it sort of peaks at about 150. The same goes for the acc (accuracy).</p>
<pre class="r"><code>history &lt;- model%&gt;%
  fit(training,
      trainlabels,
      epoch = 200,
      batch_size = 32,
      validation_split = .2)
plot(history)</code></pre>
<p><img src="/post/Keras_CTG/script_files/figure-html/unnamed-chunk-10-1.png" width="672" /> <br></p>
<div id="evaluate-on-test-data" class="section level4">
<h4>Evaluate on Test Data</h4>
<p>So on this section I am going to test the model on never before seen data. As you can see the accuracy was around 85%, which is pretty good. I’ll take those odds.</p>
<pre class="r"><code>model %&gt;%
  evaluate(test, testlabels)</code></pre>
<pre><code>## $loss
## [1] 0.4455798
## 
## $acc
## [1] 0.854063</code></pre>
<p><br></p>
</div>
<div id="predictions" class="section level4">
<h4>Predictions</h4>
<p>In this section I am going to run a probability prediction. I will then run a class prediction where we will look at a confusion matrix. Then. I will tie this all together with a cbind function so we can see where we went wrong/right.</p>
<pre class="r"><code>prob &lt;- model%&gt;%
  predict_proba(test)</code></pre>
<p><br></p>
</div>
<div id="confusion-matrix" class="section level4">
<h4>Confusion Matrix</h4>
<p>We can see that it’s predicting very well on 0’s but not so well on 1 and 2’s. This is an imbalanced Data set so that could be part of the problem. I’ll have to look at how I could better balance it. No time today.</p>
<pre class="r"><code>pred &lt;- model%&gt;%
  predict_classes(test)

table(Predicted = pred, Actual = testtarget)</code></pre>
<pre><code>##          Actual
## Predicted   0   1   2
##         0 435  38  13
##         1  22  54  10
##         2   3   2  26</code></pre>
<p><br></p>
</div>
<div id="tie-it-all-together" class="section level4">
<h4>Tie it all together</h4>
<p>So here we can see line by line which ones we properly classified and which one’s we didn’t. If this was a real dataset we could dive down and see what’s going on with those data points.</p>
<pre class="r"><code>options(scipen = 999)
tbl &lt;- cbind(prob, pred, testtarget)
head(tbl, 10)</code></pre>
<pre><code>##                                           pred testtarget
##  [1,] 0.992281139 0.00759423 0.0001245462    0          0
##  [2,] 0.973845541 0.02545224 0.0007022002    0          0
##  [3,] 0.977911413 0.02092707 0.0011616136    0          0
##  [4,] 0.001452228 0.27970883 0.7188389897    2          2
##  [5,] 0.001363919 0.27711514 0.7215209603    2          2
##  [6,] 0.917957723 0.06746713 0.0145751508    0          1
##  [7,] 0.982212126 0.01697902 0.0008088684    0          0
##  [8,] 0.984286726 0.01505913 0.0006541444    0          0
##  [9,] 0.982005358 0.01719448 0.0008001883    0          0
## [10,] 0.984929502 0.01475324 0.0003171935    0          0</code></pre>
<p><br></p>
</div>
<div id="conclusion" class="section level4">
<h4>Conclusion</h4>
<p>All in all this model outperformed the Random Forest that I did in a previous blog. The ease of use was impressive considering the API and its power. I spent about 40 minutes building it out. The biggest barrier to these Deep Learning Neural Networks is trying to explain it to non-technical people (myself included). The concepts are very difficult and abstract. But like I said previously. I’m no mathmetician. But if I can prove the model works I’m fine with that – for now.</p>
<p>Next blog I am going to go over the Mnist dataset where we user Keras to classify hand written digits.</p>
<p>Thanks for coming by.</p>
<p>cs</p>
</div>
</div>
