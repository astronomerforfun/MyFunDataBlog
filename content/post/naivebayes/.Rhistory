train <- data[ind == 1, 1:21]
test <- data[ind==2, 1:21]
trainingtarget <- data[ind = 1, 22]
testtarget <- data[ind == 2, 22]
trainlabels <- to_categorical(trainingtarget)
testlabels <- to_categorical(testtarget)
print(testlabels)
summary(model)
model <- keras_model_sequential()
model %>%
layer_dense(units = 8, activation = "relu", input_shape = c(21))%>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy")
model %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history <- model%>%
fit(training,
trainlabels,
epoch = 200,
batch_size = 32,
validation_split = .2)
history <- model%>%
fit(train,
trainlabels,
epoch = 200,
batch_size = 32,
validation_split = .2)
set.seed(1234)
ind <- sample(2, nrow(data), replace = T, prob = c(.7,.3))
training <- data[ind == 1, 1:21]
test <- data[ind==2, 1:21]
trainingtarget <- data[ind = 1, 22]
testtarget <- data[ind == 2, 22]
trainlabels <- to_categorical(trainingtarget)
testlabels <- to_categorical(testtarget)
model <- keras_model_sequential()
model %>%
layer_dense(units = 8, activation = "relu", input_shape = c(21))%>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history <- model%>%
fit(train,
trainlabels,
epoch = 200,
batch_size = 32,
validation_split = .2)
history <- model%>%
fit(training,
trainlabels,
epoch = 200,
batch_size = 32,
validation_split = .2)
trainlabels <- to_categorical(trainingtarget)
testlabels <- to_categorical(testtarget)
model <- keras_model_sequential()
model %>%
layer_dense(units = 8, activation = "relu", input_shape = c(21))%>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history <- model%>%
fit(training,
trainlabels,
epoch = 200,
batch_size = 32,
validation_split = .2)
history <- model%>%
fit(training,
trainlabels,
epoch = 200,
batch_size = 32,
validation_split = .2)
training
head(trainlabels)
set.seed(1234)
ind <- sample(2, nrow(data), replace = T, prob = c(.7,.3))
training <- data[ind == 1, 1:21]
test <- data[ind==2, 1:21]
trainingtarget <- data[ind == 1, 22]
testtarget <- data[ind == 2, 22]
trainlabels <- to_categorical(trainingtarget)
testlabels <- to_categorical(testtarget)
model <- keras_model_sequential()
model %>%
layer_dense(units = 8, activation = "relu", input_shape = c(21))%>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history <- model%>%
fit(training,
trainlabels,
epoch = 200,
batch_size = 32,
validation_split = .2)
model %>%
evaluate(test, testlabels)
prob <- model%>%
predict_proba()
prob <- model%>%
predict_proba(test)
pred <- model%>%
predict_classes(test)
table(Predicted = pred, Actual = testtarget)
cbind(prob, pred, testtarget)
prob <- model%>%
predict_proba(test)
pred <- model%>%
predict_classes(test)
table(Predicted = pred, Actual = testtarget)
prob <- model%>%
predict_proba(test)
pred <- model%>%
predict_classes(test)
table(Predicted = pred, Actual = testtarget)
cbind(prob, pred, testtarget)
head(prob)
head(pred)
cbind(prob, pred, testtarget)
prob <- model%>%
predict_proba(test)
pred <- model%>%
predict_classes(test)
table(Predicted = pred, Actual = testtarget)
cbind(prob, pred, testtarget)
options(scipen = 999)
cbind(prob, pred, testtarget)
options(scipen = 999)
tbl <- cbind(prob, pred, testtarget)
head(tbl, 10)
plot(history)
data[, 1:21] <- normalize(data[,1:21])
data[,22] <- as.numeric(data[,22])-1
trainlabels <- to_categorical(trainingtarget)
testlabels <- to_categorical(testtarget)
head(trainlabels, 10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 8, activation = "relu", input_shape = c(21))%>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy")
plot(history)
prob <- model%>%
predict_proba(test)
pred <- model%>%
predict_classes(test)
table(Predicted = pred, Actual = testtarget)
pred <- model%>%
predict_classes(test)
table(Predicted = pred, Actual = testtarget)
knitr::opts_chunk$set(echo = TRUE)
library(keras)
install_keras()
model %>%
evaluate(test, testlabels)
prob <- model%>%
predict_proba(test)
pred <- model%>%
predict_classes(test)
table(Predicted = pred, Actual = testtarget)
options(scipen = 999)
tbl <- cbind(prob, pred, testtarget)
head(tbl, 10)
library(keras)
install_keras()
data <- read.csv(file.choose(), header = T)
head(data)
data <- as.matrix(data)
dimnames(data) <- NULL
data[, 1:21] <- normalize(data[,1:21])
data[,22] <- as.numeric(data[,22])-1
set.seed(1234) #for reproducibility
ind <- sample(2, nrow(data), replace = T, prob = c(.7,.3)) # This is new way of splitting data
training <- data[ind == 1, 1:21] #Independent
test <- data[ind==2, 1:21] #Independent
trainingtarget <- data[ind == 1, 22] #dependent
testtarget <- data[ind == 2, 22] #dependent
trainlabels <- to_categorical(trainingtarget)
testlabels <- to_categorical(testtarget)
head(trainlabels, 10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 8, activation = "relu", input_shape = c(21))%>%
layer_dense(units = 3, activation = "softmax")
summary(model)
model %>%
compile(loss = "categorical_crossentropy",
optimizer = "adam",
metrics = "accuracy")
history <- model%>%
fit(training,
trainlabels,
epoch = 200,
batch_size = 32,
validation_split = .2)
plot(history)
model %>%
evaluate(test, testlabels)
prob <- model%>%
predict_proba(test)
pred <- model%>%
predict_classes(test)
table(Predicted = pred, Actual = testtarget)
data <- read.csv("CTG.csv", header = T)
head(data)
setwd("C:/Users/cshockley/Desktop/Keras_CTG")
setwd("C:/Users/cshockley/Desktop/WEBSITE_BLOG")
library(blogdown)
serve_site()
library(keras)
library(ggplot2)
library(dplyr)
boston <- dataset_boston_housing()
boston
library(keras)
library(ggplot2)
library(dplyr)
boston <- dataset_boston_housing()
boston
library(keras)
library(ggplot2)
library(dplyr)
boston <- dataset_boston_housing()
boston <- dataset_boston_housing(boston)
boston <- dataset_boston_housing("boston")
boston <- data(boston)
boston <- data(BostonHousing)
data(BostonHousing)
data(BostonHousing2)
library(mlbench)
library(MASS)
data(BostonHousing)
data <- data(BostonHousing)
data
data <- BostonHousing
names(data)
mod <- lm(medv ~ ., data)
summary(mod)
pred <- predict(mod, data)
pred
error <- pred - data$medv
sqrt(mean(error ^ 2))
library(caret)
mod2 <- train(medv ~ .-medv,
data,
method = "glm",
trControl = trainControl(method = "cv",
number = 10,
verboseIter = T))
mod2
mod2 <- train(medv ~ .-medv,
data,
method = "glm",
trControl = trainControl(method = "cv",
number = 20,
verboseIter = T))
mod2
mod2 <- train(medv ~ .-medv,
data,
method = "ranger",
trControl = trainControl(method = "cv",
number = 20,
verboseIter = T))
mod2
plot(mod2)
mod2 <- train(medv ~ .-medv,
data,
treeLength = 6
method = "ranger",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
mod2 <- train(medv ~ .-medv,
data,
treeLength = 6,
method = "ranger",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
plot(mod2)
mod2
mod2 <- train(medv ~ .-medv,
data,
treeLength = 6,
method = "lm",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
mod2
mod2 <- train(medv ~ .-medv,
data,
treeLength = 6,
method = "glmnet",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
mod2
mod2 <- train(medv ~ .-medv,
data,
treeLength = 6,
method = "xboost",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
mod2 <- train(medv ~ .-medv,
data,
treeLength = 6,
method = "glm",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
mod2 <- train(medv ~ .-medv,
data,
# treeLength = 6,
method = "glm",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
mod2
coef(mod2$finalModel)
summary(mod2)
mod2 <- train(medv ~ .-medv,
data,
# treeLength = 6,
method = "ranger",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
summary(mod2)
mod2
plot(mod2)
varImp(mod2)
varImp(mod2$finalModel)
mod2 <- train(medv ~ .-medv,
data,
# treeLength = 6,
method = "randomForest",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
mod2 <- train(medv ~ .-medv,
data,
# treeLength = 6,
method = "rf",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
varImp(mod2$finalModel)
library(randomForest)
mod3 <- randomForest(medv~ ., data)
mod3
summary(mod3)
mod3
mod3 <- randomForest(medv~ ., data)
mod3
varImpPlot(mod3)
varImp(mod2)
varImp(mod2$finalModel)
mod2 <- train(medv ~ .-medv,
data,
# treeLength = 6,
method = "rf",
trControl = trainControl(method = "cv",
number = 5,
verboseIter = T))
varImp(mod2$finalModel)
knitr::opts_chunk$set(echo = TRUE)
library("caret")
library("mlbench")
library("pROC")
install.packages("pROC")
library("caret")
library("mlbench")
library("pROC")
data(Sonar)
set.seed(107)
inTrain <- createDataPartition(y = Sonar$Class, p = .75, list = FALSE)
training <- Sonar[ inTrain,]
testing <- Sonar[-inTrain,]
my_control <- trainControl(
method="boot",
number=25,
savePredictions="final",
classProbs=TRUE,
index=createResample(training$Class, 25),
summaryFunction=twoClassSummary
)
library("rpart")
library("caretEnsemble")
install.packages("caretEnsemble")
library("rpart")
library("caretEnsemble")
model_list <- caretList(
Class~., data=training,
trControl=my_control,
methodList=c("glm", "rpart")
)
library("rpart")
library("caretEnsemble")
model_list <- caretList(
Class~., data=training,
trControl=my_control,
methodList=c("glm", "rpart", "lm", "ranger")
)
library("rpart")
library("caretEnsemble")
model_list <- caretList(
Class~., data=training,
trControl=my_control,
methodList=c("glm", "rpart", "ranger")
)
p <- as.data.frame(predict(model_list, newdata=head(testing)))
print(p)
# load the library
library(mlbench)
library(caret)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the LVQ model
set.seed(7)
modelLvq <- train(diabetes~., data=PimaIndiansDiabetes, method="lvq", trControl=control)
# train the GBM model
set.seed(7)
modelGbm <- train(diabetes~., data=PimaIndiansDiabetes, method="gbm", trControl=control, verbose=FALSE)
# train the SVM model
set.seed(7)
modelSvm <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", trControl=control)
# collect resamples
results <- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm))
# summarize the distributions
summary(results)
# boxplots of results
bwplot(results)
# dot plots of results
dotplot(results)
# load the library
library(mlbench)
library(caret)
# load the dataset
data(PimaIndiansDiabetes)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the LVQ model
set.seed(7)
modelLvq <- train(diabetes~., data=PimaIndiansDiabetes, method="lvq", trControl=control)
# train the GBM model
set.seed(7)
modelGbm <- train(diabetes~., data=PimaIndiansDiabetes, method="gbm", trControl=control, verbose=FALSE)
# train the SVM model
set.seed(7)
modelSvm <- train(diabetes~., data=PimaIndiansDiabetes, method="svmRadial", trControl=control)
set.seed(7)
modelrng <- train(diabetes~., data=PimaIndiansDiabetes, method="ranger", trControl=control)
# collect resamples
results <- resamples(list(LVQ=modelLvq, GBM=modelGbm, SVM=modelSvm, Forest = modelrng))
# summarize the distributions
summary(results)
# boxplots of results
bwplot(results)
# dot plots of results
dotplot(results)
summary(results)
bwplot(results)
dotplot(results)
?PimaIndiansDiabetes
str(PimaIndiansDiabetes)
summary(results)
bwplot(results)
install.packages("naivebayes")
knitr::opts_chunk$set(echo = TRUE)
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
setwd("C:/Users/cshockley/Desktop/naivebayes")
data <- read.csv("binary.csv", header = T)
str(data)
xtabs(~admit + rank, data)
knitr::opts_chunk$set(echo = TRUE)
library(naivebayes)
library(dplyr)
library(ggplot2)
library(psych)
data <- read.csv("binary.csv", header = T)
str(data)
xtabs(~admit + rank, data)
data$rank <- as.factor(data$rank)
data$admit <- as.factor(data$rank)
pairs.panels(data[-1])
data%>%
ggplot(aes(admit, gre, fill = admit)) + geom_boxplot() + ggtitle("Box Plot")
