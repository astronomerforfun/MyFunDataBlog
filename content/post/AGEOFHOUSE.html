---
title: "Multiple Model Comparison on Boston Housing Data"
author: "cms"
date: '2019-04-14T10:58:08-04:00'
output: html_document
featured_image: "/images/Mansion.jpeg"
---



<div id="comparing-multiple-models" class="section level3">
<h3>Comparing Multiple Models:</h3>
<p>Objective:</p>
<p>The objective of this script is to predict the age of a home in the Boston Data Set using multiple Machine Learning Algorithms. We also want to The data set is comprised of 506 rows and 14 columns. The variables range from crime per capita by town to the pupil-teacher ratio by town. We will be using Age as our Response Variable.</p>
<p><br></p>
<div id="load-packages" class="section level4">
<h4>Load Packages</h4>
<p><br></p>
<p>We’re going to use the caret package and the MASS package. The MASS package holds the Boston datset and the Caret package the models we’re going to use.</p>
<p><br></p>
<pre class="r"><code>library(caret)</code></pre>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<pre class="r"><code>library(MASS)
library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages ------------------------------------------------- tidyverse 1.2.1 --</code></pre>
<pre><code>## v tibble  2.1.1       v purrr   0.3.2  
## v tidyr   0.8.3       v dplyr   0.8.0.1
## v readr   1.3.1       v stringr 1.4.0  
## v tibble  2.1.1       v forcats 0.4.0</code></pre>
<pre><code>## -- Conflicts ---------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
## x purrr::lift()   masks caret::lift()
## x dplyr::select() masks MASS::select()</code></pre>
<p><br></p>
</div>
<div id="split-the-data-into-training-and-test-sets" class="section level4">
<h4>Split the Data into Training and Test Sets</h4>
<p>There are a variety of different ways to do this in R. I’m going to show you the Caret way. There’s a function called createDatapartition. It’s pretty handy. First we’ll store the Boston Data Set in an object called df. After we will create the rows for the Training and Test set. We will do a 70/30 Train/Test Split.</p>
<pre class="r"><code># load the data set into df
df &lt;- Boston

# store rows for data partition in partition
partition &lt;- createDataPartition(df$age, times = 1, p = .7, list = F)

#create training set
train &lt;- df[partition,]

#subtract out partition, which will leave remaining 30% of the rows.

test &lt;- df[-partition,]</code></pre>
</div>
<div id="build-out-models" class="section level4">
<h4>Build out Models</h4>
<p><br></p>
<p>We’re going to run a Random Forest, the classic Linear Regression model, and a GBM (Gradient Boosting Model). It’s easy to run these simultaneously by copy and pasting the code. The only change we’re going to make is to the method call, which specifies the algorithm we want to use.</p>
<p>We will use Repeated Cross Validation 10 x and repeat that process 2x. This will make sure that our results are well tested.</p>
<p><br></p>
<pre class="r"><code>modranger &lt;- train(age ~ ., train,
             method = &quot;ranger&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2))
modlm &lt;- train(age ~ ., train,
             method = &quot;lm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2))

modgbm &lt;- modlm &lt;- train(age ~ ., train,
             method = &quot;gbm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2))</code></pre>
</div>
<div id="lets-look-at-them-side-by-side-now." class="section level4">
<h4>Let’s look at them side by side now.</h4>
<p>We will use a call in the caret package called resamples. This will let us compare the three side by side. We will plot the models.</p>
<pre class="r"><code>smpl &lt;- resamples(list(Forest = modranger, LM = modlm, GBM = modgbm))

bwplot(smpl)</code></pre>
<p><img src="/post/AGEOFHOUSE_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>dotplot(smpl)</code></pre>
<p><img src="/post/AGEOFHOUSE_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><br></p>
<p>We can also look at the data using the summary function.</p>
<p><br></p>
<pre class="r"><code>summary(smpl)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = smpl)
## 
## Models: Forest, LM, GBM 
## Number of resamples: 20 
## 
## MAE 
##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Forest 7.390659 8.113085 9.527277 9.400747 10.16971 12.11349    0
## LM     6.055242 8.429066 9.649177 9.449090 10.59481 12.11978    0
## GBM    6.055242 8.429066 9.649177 9.449090 10.59481 12.11978    0
## 
## RMSE 
##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Forest 9.433992 10.94764 12.24808 12.47180 13.91079 15.91382    0
## LM     8.280006 10.93242 12.24437 12.38395 13.46871 17.45272    0
## GBM    8.280006 10.93242 12.24437 12.38395 13.46871 17.45272    0
## 
## Rsquared 
##             Min.   1st Qu.   Median      Mean   3rd Qu.      Max. NA&#39;s
## Forest 0.7031364 0.7713673 0.826898 0.8094070 0.8546411 0.8847218    0
## LM     0.6799745 0.7874635 0.822313 0.8116029 0.8629342 0.9088572    0
## GBM    0.6799745 0.7874635 0.822313 0.8116029 0.8629342 0.9088572    0</code></pre>
<p><br></p>
<p>We can see above that the Random Forest Model performed the best when looking at the mean RMSE. They were all really close though, which usually isn’t the case.</p>
<p>So let’s run the models again this time using some preprocesing steps.</p>
<p><br></p>
</div>
<div id="centerscalepca" class="section level4">
<h4>Center/Scale/PCA</h4>
<p><br></p>
<p>Adding Preprocessing steps in caret is easy. We wil lbe centering the data, scaling it and performing Principle Component Analysis on the variables to prevent collinearity. We may lose some ground on the RMSE but our model will be more stable.</p>
<p><br></p>
<pre class="r"><code>modranger &lt;- train(age ~ ., train,
             method = &quot;ranger&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2),
             preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))
modlm &lt;- train(age ~ ., train,
             method = &quot;lm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2),
             preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))

modgbm &lt;- train(age ~ ., train,
             method = &quot;gbm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2),
             preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))</code></pre>
<pre class="r"><code>smpl2 &lt;- resamples(list(Forest = modranger, LM = modlm, GBM = modgbm))

bwplot(smpl)</code></pre>
<p><img src="/post/AGEOFHOUSE_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p><br></p>
<p>Looks like they’re pretty close again. To get a better idea we need to look at the actual data.</p>
<p><br></p>
<pre class="r"><code>summary(smpl2)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = smpl2)
## 
## Models: Forest, LM, GBM 
## Number of resamples: 20 
## 
## MAE 
##             Min.   1st Qu.    Median      Mean  3rd Qu.     Max. NA&#39;s
## Forest  7.656452  9.157861  9.561030  9.739100 10.36060 11.65431    0
## LM     10.123333 11.613576 12.933062 12.789768 14.14492 15.70733    0
## GBM     6.954337  8.496152  9.829291  9.711801 10.46121 13.15652    0
## 
## RMSE 
##             Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Forest 10.912366 11.74690 12.63527 12.98518 13.95594 16.44129    0
## LM     13.115493 14.38446 16.57359 16.57518 18.36082 20.62310    0
## GBM     8.890768 11.16637 12.18325 13.02419 14.79535 19.19366    0
## 
## Rsquared 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## Forest 0.6477432 0.7594353 0.8132526 0.7951697 0.8491388 0.8747016    0
## LM     0.5532086 0.5977890 0.6601844 0.6637739 0.7136517 0.7998411    0
## GBM    0.5440135 0.7477256 0.8187918 0.7847244 0.8435421 0.8916606    0</code></pre>
<p><br></p>
<p>As it turns out the GBM and Forest model significantly outperformed the lm model with a mean RMSE of 12.79 compared to lm’s of 15.</p>
<p><br></p>
</div>
<div id="model-on-the-test-set" class="section level4">
<h4>Model on the Test set?</h4>
<pre class="r"><code>#use the predict function using the model on the test set (data the model hasn&#39;t seen)
predrf &lt;- predict(modranger, test)

#RMSE

error &lt;- predrf - test$age

rmse &lt;- sqrt(mean(error^2))

rmse</code></pre>
<pre><code>## [1] 13.70151</code></pre>
<p><br></p>
<p>The model performed slighly worst on the Test set. This is expected.</p>
<p><br></p>
<pre class="r"><code>predlm &lt;- predict(modlm, test)

#RMSE

errorlm &lt;- predlm - test$age

rmselm &lt;- sqrt(mean(errorlm^2))

rmselm</code></pre>
<pre><code>## [1] 16.41689</code></pre>
<p><br></p>
<p>lm model didn’t perform that well. But it looks like the standard is a 2 year increase in RMSE. If my assumption holds the GBM model should perfom at an RMSE of about 15.</p>
<p><br></p>
<pre class="r"><code>predgbm &lt;- predict(modgbm, test)

#RMSE

errorgbm &lt;- predgbm - test$age

rmsegbm &lt;- sqrt(mean(errorgbm^2))

rmsegbm</code></pre>
<pre><code>## [1] 14.40174</code></pre>
<p><br></p>
<p>Well there’s a whole lot more we could derive from these models. We could look at the plots and the variable importance as it relates to the models, extract the coefficients from the lm model, etc.</p>
<p>But for tonight – that’s it.</p>
<p>Thank you for reading.</p>
<p>cs</p>
</div>
</div>
