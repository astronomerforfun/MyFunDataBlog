---
title: "Multiple Model Comparison on Boston Housing Data"
author: "cms"
date: '2019-04-14T10:58:08-04:00'
output: html_document
featured_image: "/images/Mansion.jpeg"
---



<div id="comparing-multiple-models" class="section level3">
<h3>Comparing Multiple Models:</h3>
<p>Objective:</p>
<p>The objective of this script is to predict the age of a home in the Boston Data Set for the purpose of comparing multiple algorithms.</p>
<p>Since the purpose is to compare models I’ll be spending less time looking at the actual model results.</p>
<p><em>Data Set:</em></p>
<p>The data set is comprised of 506 rows and 14 columns. The variables range from crime per capita by town to the pupil-teacher ratio by town. We will be using Age as our Response Variable.</p>
<p><br></p>
<div id="load-packages" class="section level4">
<h4>Load Packages</h4>
<p><br></p>
<p>We’re going to use the caret package and the MASS package. The MASS package holds the Boston datset and the Caret package the models we’re going to use.</p>
<p><br></p>
<p><br></p>
</div>
<div id="split-the-data-into-training-and-test-sets" class="section level4">
<h4>Split the Data into Training and Test Sets</h4>
<p>There are a variety of different ways to do this in R. I’m going to show you the Caret way. There’s a function called createDatapartition. It’s pretty handy. First we’ll store the Boston Data Set in an object called df. After we will create the rows for the Training and Test set. We will do a 70/30 Train/Test Split.</p>
<pre class="r"><code># load the data set into df
df &lt;- Boston

# store rows for data partition in partition
partition &lt;- createDataPartition(df$age, times = 1, p = .7, list = F)

#create training set
train &lt;- df[partition,]

#subtract out partition, which will leave remaining 30% of the rows.

test &lt;- df[-partition,]</code></pre>
</div>
<div id="build-out-models" class="section level4">
<h4>Build out Models</h4>
<p><br></p>
<p>We’re going to run a Random Forest, the classic Linear Regression model, and a GBM (Gradient Boosting Model). It’s easy to run these simultaneously by copy and pasting the code. The only change we’re going to make is to the method call, which specifies the algorithm we want to use.</p>
<p>We will use Repeated Cross Validation 10 x and repeat that process 2x. This will make sure that our results are well tested.</p>
<p><br></p>
<pre class="r"><code>modranger &lt;- train(age ~ ., train,
             method = &quot;ranger&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2))
modlm &lt;- train(age ~ ., train,
             method = &quot;lm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2))

modgbm &lt;- modlm &lt;- train(age ~ ., train,
             method = &quot;gbm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2))</code></pre>
</div>
<div id="lets-look-at-them-side-by-side-now." class="section level4">
<h4>Let’s look at them side by side now.</h4>
<p>We will use a call in the caret package called resamples. This will let us compare the three side by side. We will plot the models.</p>
<pre class="r"><code>smpl &lt;- resamples(list(Forest = modranger, LM = modlm, GBM = modgbm))

bwplot(smpl)</code></pre>
<p><img src="/post/AGEOFHOUSE_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>dotplot(smpl)</code></pre>
<p><img src="/post/AGEOFHOUSE_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><br></p>
<p>We can also look at the data using the summary function.</p>
<p><br></p>
<pre class="r"><code>summary(smpl)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = smpl)
## 
## Models: Forest, LM, GBM 
## Number of resamples: 20 
## 
## MAE 
##            Min.  1st Qu.   Median     Mean   3rd Qu.     Max. NA&#39;s
## Forest 6.414857 8.420224 8.840809 9.108707  9.981752 11.58375    0
## LM     7.145296 8.649197 9.085303 9.337941 10.115155 11.88021    0
## GBM    7.145296 8.649197 9.085303 9.337941 10.115155 11.88021    0
## 
## RMSE 
##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Forest 8.287707 11.39791 12.43312 12.44734 13.85908 15.99809    0
## LM     9.251272 11.24877 12.35774 12.80533 14.49662 16.96193    0
## GBM    9.251272 11.24877 12.35774 12.80533 14.49662 16.96193    0
## 
## Rsquared 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## Forest 0.6752715 0.7852626 0.8022922 0.8082983 0.8427229 0.9244584    0
## LM     0.6614261 0.7472671 0.8232737 0.7978763 0.8446497 0.9013233    0
## GBM    0.6614261 0.7472671 0.8232737 0.7978763 0.8446497 0.9013233    0</code></pre>
<p><br></p>
<p>We can see above that the Random Forest Model performed the best when looking at the mean RMSE. They were all really close though, which usually isn’t the case.</p>
<p>So let’s run the models again this time using some preprocesing steps.</p>
<p><br></p>
</div>
<div id="centerscalepca" class="section level4">
<h4>Center/Scale/PCA</h4>
<p><br></p>
<p>Adding Preprocessing steps in caret is easy. We will be centering the data, scaling and performing Principle Component Analysis on the variables to prevent collinearity. We may lose some ground on the RMSE but our model will be more stable.</p>
<p><br></p>
<pre class="r"><code>modranger &lt;- train(age ~ ., train,
             method = &quot;ranger&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2),
             preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))</code></pre>
<pre><code>## Warning: model fit failed for Fold05.Rep1: mtry=10, min.node.size=5, splitrule=variance Error in ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,  : 
##   User interrupt or internal error.</code></pre>
<pre><code>## Warning: model fit failed for Fold05.Rep1: mtry=10, min.node.size=5, splitrule=extratrees Error in ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,  : 
##   User interrupt or internal error.</code></pre>
<pre><code>## Warning: model fit failed for Fold10.Rep1: mtry=10, min.node.size=5, splitrule=variance Error in ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,  : 
##   User interrupt or internal error.</code></pre>
<pre><code>## Warning: model fit failed for Fold10.Rep1: mtry=10, min.node.size=5, splitrule=extratrees Error in ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,  : 
##   User interrupt or internal error.</code></pre>
<pre><code>## Warning: model fit failed for Fold01.Rep2: mtry=10, min.node.size=5, splitrule=variance Error in ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,  : 
##   User interrupt or internal error.</code></pre>
<pre><code>## Warning: model fit failed for Fold01.Rep2: mtry=10, min.node.size=5, splitrule=extratrees Error in ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,  : 
##   User interrupt or internal error.</code></pre>
<pre><code>## Warning: model fit failed for Fold10.Rep2: mtry=10, min.node.size=5, splitrule=variance Error in ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,  : 
##   User interrupt or internal error.</code></pre>
<pre><code>## Warning: model fit failed for Fold10.Rep2: mtry=10, min.node.size=5, splitrule=extratrees Error in ranger::ranger(dependent.variable.name = &quot;.outcome&quot;, data = x,  : 
##   User interrupt or internal error.</code></pre>
<pre><code>## Warning in nominalTrainWorkflow(x = x, y = y, wts = weights, info =
## trainInfo, : There were missing values in resampled performance measures.</code></pre>
<pre class="r"><code>modlm &lt;- train(age ~ ., train,
             method = &quot;lm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2),
             preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))

modgbm &lt;- train(age ~ ., train,
             method = &quot;gbm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2),
             preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))</code></pre>
<pre class="r"><code>smpl2 &lt;- resamples(list(Forest = modranger, LM = modlm, GBM = modgbm))

bwplot(smpl)</code></pre>
<p><img src="/post/AGEOFHOUSE_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p><br></p>
<p>Looks like they’re pretty close again. To get a better idea we need to look at the actual data.</p>
<p><br></p>
<pre class="r"><code>summary(smpl2)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = smpl2)
## 
## Models: Forest, LM, GBM 
## Number of resamples: 20 
## 
## MAE 
##            Min.   1st Qu.    Median      Mean  3rd Qu.     Max. NA&#39;s
## Forest 7.680629  9.228789 10.488586 10.084051 11.06983 11.73962    4
## LM     9.432702 11.476054 12.994335 12.606761 13.86231 14.95804    0
## GBM    7.469150  9.417509  9.786168  9.975467 10.85735 12.45839    0
## 
## RMSE 
##             Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Forest  9.450798 11.52952 13.72013 13.22234 14.42655 17.86451    4
## LM     12.816119 13.91797 16.41526 16.17923 18.01574 20.45931    0
## GBM     9.676825 12.17686 12.89040 13.61001 15.20207 18.23906    0
## 
## Rsquared 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## Forest 0.6067091 0.7644791 0.7952587 0.7934778 0.8287019 0.8838796    4
## LM     0.5312258 0.6313185 0.6867887 0.6855850 0.7384793 0.8072988    0
## GBM    0.5844936 0.7101351 0.8028564 0.7730046 0.8422553 0.8913253    0</code></pre>
<p><br></p>
<p>As it turns out the GBM and Forest model significantly outperformed the lm model with a mean RMSE of 12.79 compared to lm’s of 15.</p>
<p><br></p>
</div>
<div id="model-on-the-test-set" class="section level4">
<h4>Model on the Test set?</h4>
<pre class="r"><code>#use the predict function using the model on the test set (data the model hasn&#39;t seen)
predrf &lt;- predict(modranger, test)

#RMSE

error &lt;- predrf - test$age

rmse &lt;- sqrt(mean(error^2))

rmse</code></pre>
<pre><code>## [1] 13.22302</code></pre>
<p><br></p>
<p>The model performed slighly worst on the Test set. This is expected.</p>
<p><br></p>
<pre class="r"><code>predlm &lt;- predict(modlm, test)

#RMSE

errorlm &lt;- predlm - test$age

rmselm &lt;- sqrt(mean(errorlm^2))

rmselm</code></pre>
<pre><code>## [1] 16.766</code></pre>
<p><br></p>
<p>lm model didn’t perform that well. But it looks like the standard is a 2 year increase in RMSE. If my assumption holds the GBM model should perfom at an RMSE of about 15.</p>
<p><br></p>
<pre class="r"><code>predgbm &lt;- predict(modgbm, test)

#RMSE

errorgbm &lt;- predgbm - test$age

rmsegbm &lt;- sqrt(mean(errorgbm^2))

rmsegbm</code></pre>
<pre><code>## [1] 13.27098</code></pre>
<p><br></p>
<p>In conclusion, this method is a great way to compare multiple algorithms to see which performs best given the dataset. Of course there’s a whole lot more information we could derive from these models as the job requires. We could look at the plots and the variable importance, extract the coefficients from the lm model, etc.</p>
<p>But for tonight – that’s it.</p>
<p>Thank you for reading.</p>
<p>cs</p>
</div>
</div>
