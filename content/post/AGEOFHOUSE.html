---
title: "Multiple Model Comparison on Boston Housing Data"
author: "cms"
date: '2019-04-14T10:58:08-04:00'
output: html_document
featured_image: "/images/Mansion.jpeg"
---



<div id="comparing-multiple-models" class="section level3">
<h3>Comparing Multiple Models:</h3>
<p>Objective:</p>
<p>The objective of this script is to predict the age of a home in the Boston Data Set for the purpose of comparing multiple algorithms.</p>
<p>Since the purpose is to compare models I’ll be spending less time looking at the actual model results.</p>
<p><em>Data Set:</em></p>
<p>The data set is comprised of 506 rows and 14 columns. The variables range from crime per capita by town to the pupil-teacher ratio by town. We will be using Age as our Response Variable.</p>
<p><br></p>
<div id="load-packages" class="section level4">
<h4>Load Packages</h4>
<p><br></p>
<p>We’re going to use the caret package and the MASS package. The MASS package holds the Boston datset and the Caret package the models we’re going to use.</p>
<p><br></p>
<p><br></p>
</div>
<div id="split-the-data-into-training-and-test-sets" class="section level4">
<h4>Split the Data into Training and Test Sets</h4>
<p>There are a variety of different ways to do this in R. I’m going to show you the Caret way. There’s a function called createDatapartition. It’s pretty handy. First we’ll store the Boston Data Set in an object called df. After we will create the rows for the Training and Test set. We will do a 70/30 Train/Test Split.</p>
<pre class="r"><code># load the data set into df
df &lt;- Boston

# store rows for data partition in partition
partition &lt;- createDataPartition(df$age, times = 1, p = .7, list = F)

#create training set
train &lt;- df[partition,]

#subtract out partition, which will leave remaining 30% of the rows.

test &lt;- df[-partition,]</code></pre>
</div>
<div id="build-out-models" class="section level4">
<h4>Build out Models</h4>
<p><br></p>
<p>We’re going to run a Random Forest, the classic Linear Regression model, and a GBM (Gradient Boosting Model). It’s easy to run these simultaneously by copy and pasting the code. The only change we’re going to make is to the method call, which specifies the algorithm we want to use.</p>
<p>We will use Repeated Cross Validation 10 x and repeat that process 2x. This will make sure that our results are well tested.</p>
<p><br></p>
<pre class="r"><code>modranger &lt;- train(age ~ ., train,
             method = &quot;ranger&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2))
modlm &lt;- train(age ~ ., train,
             method = &quot;lm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2))

modgbm &lt;- modlm &lt;- train(age ~ ., train,
             method = &quot;gbm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2))</code></pre>
</div>
<div id="lets-look-at-them-side-by-side-now." class="section level4">
<h4>Let’s look at them side by side now.</h4>
<p>We will use a call in the caret package called resamples. This will let us compare the three side by side. We will plot the models.</p>
<pre class="r"><code>smpl &lt;- resamples(list(Forest = modranger, LM = modlm, GBM = modgbm))

bwplot(smpl)</code></pre>
<p><img src="/post/AGEOFHOUSE_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<pre class="r"><code>dotplot(smpl)</code></pre>
<p><img src="/post/AGEOFHOUSE_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<p><br></p>
<p>We can also look at the data using the summary function.</p>
<p><br></p>
<pre class="r"><code>summary(smpl)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = smpl)
## 
## Models: Forest, LM, GBM 
## Number of resamples: 20 
## 
## MAE 
##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Forest 7.061932 8.608173 9.286817 9.456255 10.53941 11.71941    0
## LM     7.871239 8.695601 9.546433 9.685783 10.69715 11.48884    0
## GBM    7.871239 8.695601 9.546433 9.685783 10.69715 11.48884    0
## 
## RMSE 
##             Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Forest 10.622008 11.63859 12.10875 12.93618 14.00339 16.65638    0
## LM      9.613211 12.24608 13.37131 13.16942 14.17239 17.46930    0
## GBM     9.613211 12.24608 13.37131 13.16942 14.17239 17.46930    0
## 
## Rsquared 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## Forest 0.6254237 0.7392918 0.7998964 0.7850551 0.8435396 0.8949997    0
## LM     0.6355844 0.7380345 0.7832894 0.7799452 0.8229546 0.8939904    0
## GBM    0.6355844 0.7380345 0.7832894 0.7799452 0.8229546 0.8939904    0</code></pre>
<p><br></p>
<p>We can see above that the Random Forest Model performed the best when looking at the mean RMSE. They were all really close though, which usually isn’t the case.</p>
<p>So let’s run the models again this time using some preprocesing steps.</p>
<p><br></p>
</div>
<div id="centerscalepca" class="section level4">
<h4>Center/Scale/PCA</h4>
<p><br></p>
<p>Adding Preprocessing steps in caret is easy. We will be centering the data, scaling and performing Principle Component Analysis on the variables to prevent collinearity. We may lose some ground on the RMSE but our model will be more stable.</p>
<p><br></p>
<pre class="r"><code>modranger &lt;- train(age ~ ., train,
             method = &quot;ranger&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2),
             preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))
modlm &lt;- train(age ~ ., train,
             method = &quot;lm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2),
             preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))

modgbm &lt;- train(age ~ ., train,
             method = &quot;gbm&quot;,
             trControl = trainControl(method = &quot;repeatedcv&quot;,
                                      number = 10, repeats = 2),
             preProcess = c(&quot;center&quot;, &quot;scale&quot;, &quot;pca&quot;))</code></pre>
<pre class="r"><code>smpl2 &lt;- resamples(list(Forest = modranger, LM = modlm, GBM = modgbm))

bwplot(smpl)</code></pre>
<p><img src="/post/AGEOFHOUSE_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p><br></p>
<p>Looks like they’re pretty close again. To get a better idea we need to look at the actual data.</p>
<p><br></p>
<pre class="r"><code>summary(smpl2)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = smpl2)
## 
## Models: Forest, LM, GBM 
## Number of resamples: 20 
## 
## MAE 
##            Min.   1st Qu.    Median     Mean  3rd Qu.     Max. NA&#39;s
## Forest 8.149287  9.115653  9.828052  9.92066 10.83526 11.97281    0
## LM     8.318909 11.539965 12.556666 12.44978 13.53408 16.64526    0
## GBM    7.565521  9.180614 10.142710 10.29529 11.21284 16.43143    0
## 
## RMSE 
##             Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## Forest 10.603217 11.87712 13.53832 13.40123 14.41587 18.27227    0
## LM     11.239028 14.78022 16.55303 16.23950 17.44627 21.52309    0
## GBM     9.823987 11.61004 13.21419 13.88452 14.93168 22.96576    0
## 
## Rsquared 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA&#39;s
## Forest 0.6264813 0.7206410 0.7781562 0.7713592 0.8298419 0.8702178    0
## LM     0.3724783 0.6168811 0.6534034 0.6585398 0.7137893 0.8341289    0
## GBM    0.4843325 0.7055010 0.7474173 0.7512118 0.8268621 0.8570538    0</code></pre>
<p><br></p>
<p>As it turns out the GBM and Forest model significantly outperformed the lm model with a mean RMSE of 12.79 compared to lm’s of 15.</p>
<p><br></p>
</div>
<div id="model-on-the-test-set" class="section level4">
<h4>Model on the Test set?</h4>
<pre class="r"><code>#use the predict function using the model on the test set (data the model hasn&#39;t seen)
predrf &lt;- predict(modranger, test)

#RMSE

error &lt;- predrf - test$age

rmse &lt;- sqrt(mean(error^2))

rmse</code></pre>
<pre><code>## [1] 13.63614</code></pre>
<p><br></p>
<p>The model performed slighly worst on the Test set. This is expected.</p>
<p><br></p>
<pre class="r"><code>predlm &lt;- predict(modlm, test)

#RMSE

errorlm &lt;- predlm - test$age

rmselm &lt;- sqrt(mean(errorlm^2))

rmselm</code></pre>
<pre><code>## [1] 17.5307</code></pre>
<p><br></p>
<p>lm model didn’t perform that well. But it looks like the standard is a 2 year increase in RMSE. If my assumption holds the GBM model should perfom at an RMSE of about 15.</p>
<p><br></p>
<pre class="r"><code>predgbm &lt;- predict(modgbm, test)

#RMSE

errorgbm &lt;- predgbm - test$age

rmsegbm &lt;- sqrt(mean(errorgbm^2))

rmsegbm</code></pre>
<pre><code>## [1] 13.16047</code></pre>
<p><br></p>
<p>Well there’s a whole lot more we could derive from these models. We could look at the plots and the variable importance as it relates to the models, extract the coefficients from the lm model, etc.</p>
<p>But for tonight – that’s it.</p>
<p>Thank you for reading.</p>
<p>cs</p>
</div>
</div>
