---
date: 2018-11-05T10:58:08-04:00
description: "Glmnet Compared to Trusty 'lm' Model by Chris Shockley"
featured_image: "/images/lasso.jpeg"
tags: ["supervised learning", "caret", "machine learning", "models", "boston", "tutorial" ]
title: "What?  Hypertune Using glmnet and Caret by Max Kuhn?  Pass it along."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r include = F, warning=FALSE}
library(caret)
library(mlbench)
library(glmnet)
library(psych)
```
<br>

## Objective:


So in this blog I am going to go through a lesson I recently took with a gentlemen by the name Bharatendra Rai, where he goes through the glmnet model in detail.  

The reason this model is great is that you can hypretune it, which you will see momentarily.  It is an equal to the Random Forests but where glmnet takes the lead is with the amount of information you can pull from the model and its parameters.  You can also see where and which features are overfitting, which we all know is an important aspect to a healthy model.


#### Data-Set

I'm going to use the Boston Housing Data Set.  It consists of 506 observations and 14 variables.  We are going to use the median value as our *Response* variable.



```{r }
data("BostonHousing")
df <- BostonHousing
str(df)
```
<br>

#### Correlation Panel

When model building its important to try to prevent collinearity, which causes overfitting.  In the plot below you can see all the variables and how they relate to each other.  Preferrably you don't want to see variables with high correlations, those near 1.

```{r}
pairs.panels(df[c(-4, -14)], cex = 2)
```
<br>

## Split Data into Training and Test Sets

I will create a 70/30 Train/Test split.
```{r}
set.seed(222)
rows <- sample(nrow(df), nrow(df) *.7, replace = T)
train <- df[rows,]
test <- df[-rows,]
```


## Carat Package

One of the great things about the carat package is that you can create your own custom control.  Here I will use a repeated cross validation call with a 10 split, repeated 5 times.  

Essentially, it will break the training set into 10 training and test sets and calculate the RMSE on each. It will do this 5 times.  It will then average the RMSE over the 50 iterations.  This gives us a good estimate of the RMSE.  And a great idea by Max Kuhn (creator of package).
```{r}
custom <- trainControl(method = "repeatedcv", number = 10,
                       repeats = 5,
                       verboseIter = F)
```
<br>

## Run the lm Model.

Here I will run the old standby lm model.  Below you will see that the RMSE is 4.30.  
```{r}
set.seed(1234)
lm <- train(medv ~ ., 
            train,
            method = "lm",
            trControl = custom)

```
```{r}
options(scipen = 9999)

lm
```
<br>

## Glmnet Model (Ridge Regression)

*Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value.*

On this one I will use the glmnet model where I will set the alpha to 0 and the lambda with small values up to 1.  I will run it through the training set and then take a look.


I can see that the RMSE for this model is slightly higher than the lm model with an RMSE of 4.39 with 76% of the variance being explained by the features/variables.
```{r include = F}
set.seed(1234)
ridge <- train(medv ~ .,
               train,
               method = 'glmnet',
               tuneGrid = expand.grid(alpha = 0, 
                                      lambda = seq(.0001, 1, length = 5)),
               trControl = custom)
ridge
```
<br>

## Interpretation of Graphs

I can see that the lambda of .05 gives the best result. In this model.   
```{r}
plot(ridge)

```
<br> 

We can see in this plot that as lambda decreases the coefficients start to increase resulting in higher Squared Errors.  Lambda also adds a small penalty to those features that don't contribute much to the model.  So from top you can see that all 13 vaiables are in play throughout.  From right to left you can see some of the variables increasing in size, however the blue one is growing at a greater rate and negative.  In other words, variables 4 and 6 (you can see the small numbers at the far left) will have a bigger influence on the model.
```{r}

plot(ridge$finalModel, xvar = "lambda", label = T)

```
<br>
In this plot you can see that from left to right that at .2 of the variability is being explained with a slight growth in the coefficients.  And at .7 or 70% of the variance is explained.  But the coeffients are becoming much larger.  By the time it gets to 80 the coeffients grow way to large and is likely the result of overfitting.
```{r}
plot(ridge$finalModel, xvar = "dev", label = T)

```
<br>


## Variable Importance

This is great because this function allows me to see what variables are the most important in the model.  If not too important I could take them out all together or apply a penalty to them to reduce them to 0.


```{r}
plot(varImp(ridge, scale = T))
```

## Lasso Regression

*Lasso method. The only difference from Ridge regression is that the regularization term is in absolute value. ... Lasso method overcomes the disadvantage of Ridge regression by not only punishing high values of the coefficients but actually setting them to zero if they are not relevant.*

In this example I'll use an alpha of 1 fixed and a lambda between .0001 and .2.

The RMSE is now 4.3 and the optimal lambda is .0001.
```{r}
set.seed(1234)

lasso <- train(medv ~ ., 
               train,
               method = "glmnet",
               tuneGrid = expand.grid(alpha = 1,
                                      lambda = seq(.0001, .2, length = 5)),
               trControl = custom)

lasso
```
<br>

## Explanation of Graphs

I can see that a very small alpha is optimal.  That is consistent with the optimal value of lamda the model stated (.0001).
```{r}
plot(lasso)

```

<br>

I can see that asa lamda decreases the variables increase slowly.  The next chart shows what I'm looking for though. 
```{r}
plot(lasso$finalModel, xvar = "lambda", label = T)

```
<br> 

This plot shows that 60% of the variance is explained by only 3 variables.  So there are some duds features in the model or collinearity.
```{r}
plot(lasso$finalModel, xvar = "dev", label = T)

```

<br>

How about Variable Importance?


```{r}
plot(varImp(lasso, scale = T))
```
<br>

## Now how about a combined model where we do ridge and lasso regression otherwise known as Net Regression, hence the name of the package "glmnet"?

Now we have an RMSE of 4.3 an optimal alpha of .1111 and lamda of .0001.  Best model yet.  Now I'm going to skip the graphs this time as they are relatively the same.  But I am going to compile the models for comparison.
```{r include = F}
set.seed(1234)
en <- train(medv ~ ., 
               train,
               method = "glmnet",
               tuneGrid = expand.grid(alpha = seq(0,1, length = 10),
                                      lambda = seq(.0001, .5, length = 5)),
               trControl = custom)
en$bestTune
```

<br>

## Comparison

Turns out that Lasso Regression had the best RMSE and the Net model had the best R Squared (Look at the Median Column).  All of them were very close.  I was surprised at how well the lm model held its own.

```{r}
model_list <- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res <- resamples(model_list)
summary(res)

```

<br> 

Box plot of the models.  It's hard to tell though which is best since they are so close.  The table above gives a more precise metric.

```{r}
bwplot(res, metric = "RMSE")
```

<br>

## Future Analysis

The only thing left to do is to predict on the test set using the model.  I will do that in the future.  I suspect that the RMSE will be slightly higher than the models but not much.  


## Conclusion:

I learned a great deal through his course and teaching it back to myself and you.  I now have a greater understanding of how to tune models.  I will practice more in future blogs.  Glmnet is my goto when I need more information then a Random Forest can offer.  
