---
date: 2018-11-5T10:58:08-04:00
description: "Glmnet Compared to trusty 'lm' Model by Chris Shockley"
featured_image: "/images/lasso.jpeg"
tags: ["unsupervised learning", "k means", "machine learning" ]
title: "What?  Hypertune Using glmnet and Caret by Max Kuhn?  Yep.  Pass it along."
---



<div id="glmnet-package" class="section level2">
<h2>glmnet package</h2>
<p><br></p>
</div>
<div id="objective" class="section level2">
<h2>Objective:</h2>
<p>So in this blog I am going to go through a lesson I recently took with a gentlemen by the name Bharatendra Rai, where he goes through the glmnet model in detail.</p>
<p>The reason this model is great is that you can hypretune it, which you will see momentarily. It is an equal to the Random Forests but where glmnet takes the lead is with the amount of information you can pull from the model and its parameters. You can also see where and which features are overfitting, which you is an important aspect to a healthy model.</p>
<div id="data-set" class="section level4">
<h4>Data-Set</h4>
<p>I’m going to use the Boston Housing Data Set. It consists of 506 observations and 14 variables. We are going to use the median value as our <em>Response</em> variable.</p>
<pre class="r"><code>data(&quot;BostonHousing&quot;)
df &lt;- BostonHousing
str(df)</code></pre>
<pre><code>## &#39;data.frame&#39;:    506 obs. of  14 variables:
##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...
##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...
##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...
##  $ chas   : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ...
##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...
##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...
##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...
##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...
##  $ rad    : num  1 2 2 3 3 3 5 5 5 5 ...
##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...
##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...
##  $ b      : num  397 397 393 395 397 ...
##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...
##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...</code></pre>
<p><br></p>
</div>
<div id="correlation-panel" class="section level4">
<h4>Correlation Panel</h4>
<p>When model building its important to try to prevent collinearity, which causes overfitting. In the plot below you can see all the variables and how they relate to each other. Preferrably you don’t want to see variables with high correlations, those near 1.</p>
<pre class="r"><code>pairs.panels(df[c(-4, -14)], cex = 2)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-3-1.png" width="672" /> <br></p>
</div>
</div>
<div id="split-data-into-training-and-test-sets" class="section level2">
<h2>Split Data into Training and Test Sets</h2>
<p>I will create a 70/30 Train/Test split.</p>
<pre class="r"><code>set.seed(222)
rows &lt;- sample(nrow(df), nrow(df) *.7, replace = T)
train &lt;- df[rows,]
test &lt;- df[-rows,]</code></pre>
</div>
<div id="carat-package" class="section level2">
<h2>Carat Package</h2>
<p>One of the great things about the carat package is that you can create your own custom control. Here I will use a repeated cross validation call with a 10 split, repeated 5 times.</p>
<p>Essentially, it will break the training set into 10 training and test sets and calculate the RMSE on each. It will do this 5 times. It will then average the RMSE over the 50 iterations. This gives us a good estimate of the RMSE. And a great idea by Max Kuhn (creator of package).</p>
<pre class="r"><code>custom &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10,
                       repeats = 5,
                       verboseIter = F)</code></pre>
<p><br></p>
</div>
<div id="run-the-lm-model." class="section level2">
<h2>Run the lm Model.</h2>
<p>Here I will run the old standby lm model. Below you will see that the RMSE is 4.30.</p>
<pre class="r"><code>set.seed(1234)
lm &lt;- train(medv ~ ., 
            train,
            method = &quot;lm&quot;,
            trControl = custom)</code></pre>
<pre class="r"><code>options(scipen = 9999)

lm</code></pre>
<pre><code>## Linear Regression 
## 
## 354 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 319, 318, 319, 318, 319, 319, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   4.303338  0.7678331  3.185276
## 
## Tuning parameter &#39;intercept&#39; was held constant at a value of TRUE</code></pre>
<p><br></p>
</div>
<div id="glmnet-model-ridge-regression" class="section level2">
<h2>Glmnet Model (Ridge Regression)</h2>
<p><em>Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity. When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value.</em></p>
<p>On this one I will use the glmnet model where I will set the alpha to 0 and the lambda with small values up to 1. I will run it through the training set and then take a look.</p>
<p>I can see that the RMSE for this model is slightly higher than the lm model with an RMSE of 4.39 with 76% of the variance being explained by the features/variables.</p>
<p><br></p>
</div>
<div id="interpretation-of-graphs" class="section level2">
<h2>Interpretation of Graphs</h2>
<p>I can see that the lambda of .05 gives the best result. In this model.</p>
<pre class="r"><code>plot(ridge)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-9-1.png" width="672" /> <br></p>
<p>We can see in this plot that as lambda decreases the coefficients start to increase resulting in higher Squared Errors. Lambda also adds a small penalty to those features that don’t contribute much to the model. So from top you can see that all 13 vaiables are in play throughout. From right to left you can see some of the variables increasing in size, however the blue one is growing at a greater rate and negative. In other words, variables 4 and 6 (you can see the small numbers at the far left) will have a bigger influence on the model.</p>
<pre class="r"><code>plot(ridge$finalModel, xvar = &quot;lambda&quot;, label = T)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-10-1.png" width="672" /> <br> In this plot you can see that from left to right that at .2 of the variability is being explained with a slight growth in the coefficients. And at .7 or 70% of the variance is explained. But the coeffients are becoming much larger. By the time it gets to 80 the coeffients grow way to large and is likely the result of overfitting.</p>
<pre class="r"><code>plot(ridge$finalModel, xvar = &quot;dev&quot;, label = T)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-11-1.png" width="672" /> <br></p>
</div>
<div id="variable-importance" class="section level2">
<h2>Variable Importance</h2>
<p>This is great because this function allows me to see what variables are the most important in the model. If not too important I could take them out all together or apply a penalty to them to reduce them to 0.</p>
<pre class="r"><code>plot(varImp(ridge, scale = T))</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="lasso-regression" class="section level2">
<h2>Lasso Regression</h2>
<p><em>Lasso method. The only difference from Ridge regression is that the regularization term is in absolute value. … Lasso method overcomes the disadvantage of Ridge regression by not only punishing high values of the coefficients but actually setting them to zero if they are not relevant.</em></p>
<p>In this example I’ll use an alpha of 1 fixed and a lambda between .0001 and .2.</p>
<p>The RMSE is now 4.3 and the optimal lambda is .0001.</p>
<pre class="r"><code>set.seed(1234)

lasso &lt;- train(medv ~ ., 
               train,
               method = &quot;glmnet&quot;,
               tuneGrid = expand.grid(alpha = 1,
                                      lambda = seq(.0001, .2, length = 5)),
               trControl = custom)

lasso</code></pre>
<pre><code>## glmnet 
## 
## 354 samples
##  13 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 5 times) 
## Summary of sample sizes: 319, 318, 319, 318, 319, 319, ... 
## Resampling results across tuning parameters:
## 
##   lambda    RMSE      Rsquared   MAE     
##   0.000100  4.303214  0.7677303  3.175517
##   0.050075  4.317879  0.7654724  3.134688
##   0.100050  4.375287  0.7585802  3.142376
##   0.150025  4.466113  0.7482112  3.191947
##   0.200000  4.507634  0.7437641  3.206663
## 
## Tuning parameter &#39;alpha&#39; was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.0001.</code></pre>
<p><br></p>
</div>
<div id="explanation-of-graphs" class="section level2">
<h2>Explanation of Graphs</h2>
<p>I can see that a very small alpha is optimal. That is consistent with the optimal value of lamda the model stated (.0001).</p>
<pre class="r"><code>plot(lasso)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p><br></p>
<p>I can see that asa lamda decreases the variables increase slowly. The next chart shows what I’m looking for though.</p>
<pre class="r"><code>plot(lasso$finalModel, xvar = &quot;lambda&quot;, label = T)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-15-1.png" width="672" /> <br></p>
<p>This plot shows that 60% of the variance is explained by only 3 variables. So there are some duds features in the model or collinearity.</p>
<pre class="r"><code>plot(lasso$finalModel, xvar = &quot;dev&quot;, label = T)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<p><br></p>
<p>How about Variable Importance?</p>
<pre class="r"><code>plot(varImp(lasso, scale = T))</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-17-1.png" width="672" /> <br></p>
</div>
<div id="now-how-about-a-combined-model-where-we-do-ridge-and-lasso-regression-otherwise-known-as-net-regression-hence-the-name-of-the-package-glmnet" class="section level2">
<h2>Now how about a combined model where we do ridge and lasso regression otherwise known as Net Regression, hence the name of the package “glmnet”?</h2>
<p>Now we have an RMSE of 4.3 an optimal alpha of .1111 and lamda of .0001. Best model yet. Now I’m going to skip the graphs this time as they are relatively the same. But I am going to compile the models for comparison.</p>
<p><br></p>
</div>
<div id="comparison" class="section level2">
<h2>Comparison</h2>
<p>Turns out that Lasso Regression had the best RMSE and the Net model had the best R Squared (Look at the Median Column). All of them were very close. I was surprised at how well the lm model held its own.</p>
<pre class="r"><code>model_list &lt;- list(LinearModel = lm, Ridge = ridge, Lasso = lasso, ElasticNet = en)
res &lt;- resamples(model_list)
summary(res)</code></pre>
<pre><code>## 
## Call:
## summary.resamples(object = res)
## 
## Models: LinearModel, Ridge, Lasso, ElasticNet 
## Number of resamples: 50 
## 
## MAE 
##                 Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## LinearModel 2.352321 2.899379 3.063309 3.185276 3.506306 4.062280    0
## Ridge       2.292097 2.769948 3.016595 3.076879 3.324034 4.135409    0
## Lasso       2.347178 2.895592 3.059505 3.175517 3.492517 4.066730    0
## ElasticNet  2.348923 2.892819 3.058201 3.174246 3.492608 4.067000    0
## 
## RMSE 
##                 Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA&#39;s
## LinearModel 3.218283 3.683662 4.097799 4.303338 4.777701 6.503843    0
## Ridge       3.087577 3.652257 4.168664 4.351969 4.768355 7.076786    0
## Lasso       3.211613 3.674361 4.094753 4.303214 4.794841 6.539509    0
## ElasticNet  3.208691 3.674259 4.096475 4.302170 4.787268 6.539673    0
## 
## Rsquared 
##                  Min.   1st Qu.    Median      Mean   3rd Qu.      Max.
## LinearModel 0.5389469 0.7174655 0.7830427 0.7678331 0.8221920 0.8896646
## Ridge       0.4590974 0.7049716 0.7888941 0.7624169 0.8248683 0.8992497
## Lasso       0.5360178 0.7158311 0.7838163 0.7677303 0.8220441 0.8903956
## ElasticNet  0.5355595 0.7165078 0.7836337 0.7678382 0.8222248 0.8904117
##             NA&#39;s
## LinearModel    0
## Ridge          0
## Lasso          0
## ElasticNet     0</code></pre>
<p><br></p>
<p>Box plot of the models. It’s hard to tell though which is best since they are so close. The table above gives a more precise metric.</p>
<pre class="r"><code>bwplot(res, metric = &quot;RMSE&quot;)</code></pre>
<p><img src="/post/glmnet/glmnet_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<p><br></p>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion:</h2>
<p>I learned a great deal through his course and teaching it back to myself and you. I now have a greater understanding of how to tune models. I will practice more in future blogs. Glmnet is my goto when I need more information then a Random Forest can offer.</p>
</div>
