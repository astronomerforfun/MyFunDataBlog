# store the result of the loss function.  We will plot this later
loss_df$loss[i] <- loss_function(my_nn)
}
# print the predicted outcome next to the actual outcome
data.frame(
"Predicted" = round(my_nn$output, 3),
"Actual" = y
)
##   Predicted Actual
## 1     0.017      0
## 2     0.975      1
## 3     0.982      1
## 4     0.024      0
Above you can see that the predicted values are fairly close to the actual observed values.
The plot below displays the result of the loss function as the model is trained. The objective of the model training is to minimize the result of the loss function (Y axis), and we can see that as we progress in iterations (X axis), the result of the loss function approaches zero.
# plot the cost
library(ggplot2)
ggplot(data = loss_df, aes(x = iteration, y = loss)) +
geom_line()
ggplot(data = loss_df, aes(x = iteration, y = loss)) +
geom_line()
Create Training Data
First, we create the data to train the neural network.
# predictor variables
X <- matrix(c(
0,0,3,
0,13,1,
1,3,1,
1,4,1
),
ncol = 3,
byrow = TRUE
)
# observed outcomes
y <- c(0, 1, 1, 0)
# print the data so we can take a quick look at it
cbind(X, y)
##            y
## [1,] 0 0 1 0
## [2,] 0 1 1 1
## [3,] 1 0 1 1
## [4,] 1 1 1 0
The above data contains 4 observations; each row is an observation. The 3 columns on the left (i.e. the X object) are used with the observed outcomes (“y” column on the right) to train the model. Once our model is trained, we will be able to pass new predictor variables (i.e. new rows of X) to the model and predict y values that have not yet been observed.
Create an object to store the state of our neural network
Now that we have our data, we need to create the model. First, we create an object to store the state of the model.
# generate a random value between 0 and 1 for each
# element in X.  This will be used as our initial weights
# for layer 1
rand_vector <- runif(ncol(X) * nrow(X))
# convert above vector into a matrix
rand_matrix <- matrix(
rand_vector,
nrow = ncol(X),
ncol = nrow(X),
byrow = TRUE
)
# this list stores the state of our neural net as it is trained
my_NN <- list(
# predictor variables
input = X,
# weights for layer 1
weights1 = rand_matrix,
# weights for layer 2
weights2 = matrix(runif(4), ncol = 1),
# actual observed
y = y,
# stores the predicted outcome
output = matrix(
rep(0, times = 4),
ncol = 1
)
)
Activation Function
Now that we have some data to work with (X and y) and a list to store the state of our model (my_NN), we can put together the functions to train our model.
Here we define the activation function. The activation function converts a layer’s inputs to outputs. The outputs are then passed to the next layer. The Sigmoid activation function is used in our model.
#' the activation function
sigmoid <- function(x) {
1.0 / (1.0 + exp(-x))
}
#' the derivative of the activation function
sigmoid_derivative <- function(x) {
x * (1.0 - x)
}
Loss Function
The loss function is used to determine the goodness of fit of the model. We use the Sum-of-Squares Error as our loss function.
loss_function <- function(nn) {
sum((nn$y - nn$output) ^ 2)
}
The goal of the neural network is to find weights for each layer that minimize the result of the loss function.
Feedforward and Back Propagation
In order to minimize the loss function we perform feedforward and backpropagation. Feedforward applies the activation function to the layers and produces a predicted outcome.
feedforward <- function(nn) {
nn$layer1 <- sigmoid(nn$input %*% nn$weights1)
nn$output <- sigmoid(nn$layer1 %*% nn$weights2)
nn
}
Backpropagation takes the predicted outcome, resulting from the feedforward step, and adjust the layer weights to reduce the loss function.
backprop <- function(nn) {
# application of the chain rule to find derivative of the loss function with
# respect to weights2 and weights1
d_weights2 <- (
t(nn$layer1) %*%
# `2 * (nn$y - nn$output)` is the derivative of the sigmoid loss function
(2 * (nn$y - nn$output) *
sigmoid_derivative(nn$output))
)
d_weights1 <- ( 2 * (nn$y - nn$output) * sigmoid_derivative(nn$output)) %*%
t(nn$weights2)
d_weights1 <- d_weights1 * sigmoid_derivative(nn$layer1)
d_weights1 <- t(nn$input) %*% d_weights1
# update the weights using the derivative (slope) of the loss function
nn$weights1 <- nn$weights1 + d_weights1
nn$weights2 <- nn$weights2 + d_weights2
nn
}
Train the Model
We are now ready to train our model. The training process repeatedly calls feedforward() and backprop() in order to reduce the loss function.
# number of times to perform feedforward and backpropagation
n <- 1500
# data frame to store the results of the loss function.
# this data frame is used to produce the plot in the
# next code chunk
loss_df <- data.frame(
iteration = 1:n,
loss = vector("numeric", length = n)
)
for (i in seq_len(1500)) {
my_NN <- feedforward(my_NN)
my_NN <- backprop(my_NN)
# store the result of the loss function.  We will plot this later
loss_df$loss[i] <- loss_function(my_NN)
}
# print the predicted outcome next to the actual outcome
data.frame(
"Predicted" = round(my_NN$output, 3),
"Actual" = y
)
##   Predicted Actual
## 1     0.017      0
## 2     0.975      1
## 3     0.982      1
## 4     0.024      0
Above you can see that the predicted values are fairly close to the actual observed values.
The plot below displays the result of the loss function as the model is trained. The objective of the model training is to minimize the result of the loss function (Y axis), and we can see that as we progress in iterations (X axis), the result of the loss function approaches zero.
# plot the cost
library(ggplot2)
ggplot(data = loss_df, aes(x = iteration, y = loss)) +
geom_line()
data.frame(
"Predicted" = round(my_NN$output, 3),
"Actual" = y
)
df <- mpg%>%
select_if(is.numeric)
df
princomp(df)
pca <- princomp(df)
plot(pca)
cor.plot(pca)
summary(pca)
pca <- prcomp(df)
pca$x
df2 <- data.frame(pc$x[,1:3])
df2 <- data.frame(pca$x[,1:3])
plot(df2)
pairs.panels(df2)
kmeans(df2, 3)
y <- kmeans(df2, 3)
y$cluster
y$cluster <- as.factor(y$cluster)
df <- mpg%>%
select_if(as.numeric)
df <- mpg%>%
select_if(as.numeric)
df
df
m <- kmeans(df, 3)
m
m$cluster <- as.factor(m$cluster)
df2 <- cbind(m$cluster, mpg$class)
table(df2)
table(df, mpg$class)
table(m$cluster, mpg$class)
m <- kmeans(df, 7)
m$cluster <- as.factor(m$cluster)
df2 <- cbind(m$cluster, mpg$class)
table(m$cluster, mpg$class)
m <- kmeans(df, 2)
m$cluster <- as.factor(m$cluster)
df2 <- cbind(m$cluster, mpg$class)
table(m$cluster, mpg$class)
plot(m$cluster, mpg$class)
barplot(m$cluster, mpg$class)
mpg
library(ggplot2)
library(dplyr)
library(ggbiplot)
install.packages(rgl)
install.packages("rgl")
library(rgl)
df
comp <- prcomp(df)
df
summary(df)
summary(comp)
comp <- princomp(df)
summary(comp)
plot3d(comp$scores[,1:3], col = df$year)
plot3d(comp$scores[,1:3], col = df$cyl)
plot3d(comp$scores[,1:2], col = df$cyl)
comp <- princomp(df)
ggbiplot(comp)
ggbiplot(comp[,1:2])
ggbiplot(comp$scores[,1:2])
ggbiplot(comp)
comp <- princomp(mpg)
comp <- princomp(df)
ggbiplot(comp)
ggbiplot(comp, labels = df$displ)
ggbiplot(comp, labels = df$year)
ggbiplot(comp, labels = df$cyl)
diamonds
df <- diamonds%>%
select_if(as.numeric)
df <- diamonds%>%
select_if(is.numeric)
df
mod <- princomp(df)
mod
ggbiplot(mod)
mod <- princomp(df)
mod
library(ggplot2)
library(dplyr)
library(ggbiplot)
install.packages("rgl")
library(rgl)
df <- diamonds%>%
select_if(is.numeric)
mod <- princomp(df)
mod
mod <- princomp(df)
mod
install.packages("rgl")
df <- diamonds%>%
select_if(is.numeric)%>%
sample_n(100, replace = T)
df
mod <- princomp(df)
mod
ggbiplot(mod)
library(ggplot2)
library(dplyr)
library(ggbiplot)
df <- diamonds%>%
select_if(is.numeric)%>%
sample_n(100, replace = T)
df
mod <- princomp(df)
ggbiplot(mod)
names(df)
df <- df[,c(1,2,3,5,6,7)]
mod <- princomp(df)
ggbiplot(mod)
df <- df[,c(1,2,3)]
mod <- princomp(df)
ggbiplot(mod)
head(vehicle)
install.packages("neuralnet")
TKS=c(20,10,30,20,80,30)
CSS=c(90,20,40,50,50,80)
Placed=c(1,0,0,0,1,1)
# Here, you will combine multiple columns or features into a single set of data
df=data.frame(TKS,CSS,Placed)
nn <- neuralnet::neuralnet(Placed ~ TKS + CSS, data = df, hidden = 4, act.fct = "logistic",
linear.output = F)
plot(nn)
library(neuralnet)
TKS=c(20,10,30,20,80,30)
CSS=c(90,20,40,50,50,80)
Placed=c(1,0,0,0,1,1)
# Here, you will combine multiple columns or features into a single set of data
df=data.frame(TKS,CSS,Placed)
nn <- neuralnet::neuralnet(Placed ~ TKS + CSS, data = df, hidden = 4, act.fct = "logistic",
linear.output = F)
plot(nn)
TKS=c(20,10,30,20,80,30)
CSS=c(90,20,40,50,50,80)
Placed=c(1,0,0,0,1,0)
# Here, you will combine multiple columns or features into a single set of data
df=data.frame(TKS,CSS,Placed)
nn <- neuralnet::neuralnet(Placed ~ TKS + CSS, data = df, hidden = 4, act.fct = "logistic",
linear.output = F)
plot(nn)
TKS=c(30,40,85)
CSS=c(85,50,40)
test=data.frame(TKS,CSS)
Predict <- neuralnet::compute(nn,test)
Predict$net.result
nn <- neuralnet(Placed ~ TKS + CSS, data = df, hidden = 4, act.fct = "logistic",
linear.output = F)
?nn
?neuralnet
TKS=c(20,10,30,20,80,30)
CSS=c(90,20,40,50,50,80)
Placed=c(1,1,0,0,1)
# Here, you will combine multiple columns or features into a single set of data
df=data.frame(TKS,CSS,Placed)
nn <- neuralnet(Placed ~ TKS + CSS, data = df, hidden = 4, act.fct = "logistic",
linear.output = F)
plot(nn)
TKS=c(30,40,85)
CSS=c(85,50,40)
test=data.frame(TKS,CSS)
Predict <- neuralnet::compute(nn,test)
Predict$net.result
iris
iris
set.seed(500)
library(MASS)
data <- Boston
sum(is.na(x)))
sapply(data, function(x){
sum(is.na(x)))
})
sapply(data, function(x)
sum(is.na(x)))
)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(medv~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)
sapply(data, function(x)
sum(is.na(x)))
)
library(caret)
mpg
library(ggplot2)
mpg
library(dplyr)
mpg
mpg
library(caret)
library(ggplot2)
library(dplyr)
library(dplyr)
library(ggplot2)
library(caret)
install.packages("caret")
install.packages("dplyr")
install.packages("ggplot2")
library(caret)
library(ggplot2)
library(dplyr)
mpg
mpg
mtcars
library(caret)
library(ggplot2)
library(dplyr)
library(dplyr)
install.packages("tidyverse")
set.seed(500)
library(MASS)
data <- Boston
sapply(data, function(x)
sum(is.na(x)))
)
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(medv~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)
library(caret)
library(ggplot2)
library(dplyr)
set.seed(500)
library(MASS)
data <- Boston
data
y <- lm(medv ~ ., data)
y
summary(y)
?Boston
mod <- train(medv ~ ., data = data)
library(caret)
mod <- train(medv ~ ., data = data)
mod <- train(medv ~ ., data = data,
method = "ranger",
preProc = c("center", "scale"))
library(caret)
library(tidyverse)
library(caret)
mod <- train(medv ~ ., data = data,
method = "ranger",
preProc = c("center", "scale"))
mod <- train(medv ~ ., data = data,
method = "lm",
preProc = c("center", "scale"))
mod <- train(medv ~ ., data = data,
method = "lm")
mod <- train(medv ~ ., data = data,
method = "lm")
install.packages("caretEnsemble")
mod <- train(medv ~ ., data = data,
method = "lm")
library(caret)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(caret)
library(caretEnsemble)
install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
mod <- train(medv ~ ., data = data,
method = "lm")
mod <- train(medv ~ ., data = data,
method = "lm")
set.seed(500)
library(MASS)
library(caret)
data <- Boston
mod <- train(medv ~ ., data = data,
method = "lm")
knitr::opts_chunk$set(echo = TRUE)
hist(mtcars$mpg, col = "green")
?hist
hist(mtcars$mpg, col = "green", main = "Histogram of mpg in Mtcars Dataset")
hist(mtcars$mpg, col = "green", main = "Histogram of mpg in Mtcars Dataset", breaks = 30)
hist(mtcars$mpg, col = "green", main = "Histogram of mpg in Mtcars Dataset", breaks = 15)
hist(mtcars$mpg, col = "green", main = "Histogram of mpg in Mtcars Dataset", breaks = 3)
hist(mtcars$mpg, col = "green", main = "Histogram of mpg in Mtcars Dataset", breaks = 50)
hist(mtcars$mpg, col = "green", main = "Histogram of mpg in Mtcars Dataset", breaks = 30)
hist(mtcars$mpg, col = "green", main = "Histogram of mpg in Mtcars Dataset", breaks = 5)
mtcars$binary <- ifelse(mtcars$mpg > 20, 1,0)
head(mtcars)
head(mtcars$binary)
head(mtcars[,c(1,13)])
head(mtcars[,c(1,12)])
rows <- sample(1:nrow(mtcars), nrow(mtcars) * .7)
train <- mtcars[rows,]
test <- mtcars[-rows,]
test
train
nn <- neuralnet(binary ~ ., data = train,
hidden = 3,
act.fct = "logistic", linear.output = F)
nn <- neuralnet(binary ~ ., data = train,
hidden = 3,
act.fct = "logistic", linear.output = T)
library(neuralnet)
nn <- neuralnet(binary ~ ., data = train,
hidden = 3,
act.fct = "logistic", linear.output = T)
nn <- neuralnet(binary ~ ., data = train,
hidden = 3,
act.fct = "logistic", linear.output = F)
nn
nn$net.result
predict <- compute(nn, test)
predict$net.result
test.results <- ifelse(predict$net.result > .5, 1,0)
test.results
table(test.results, test)
table(test.results, test$binary)
setwd("C:/Users/cshockley/Desktop/WEBSITE_BLOG")
library(blogdown)
serve_site()
plot(nn)
knitr::opts_chunk$set(echo = TRUE)
library(neuralnet)
hist(mtcars$mpg, col = "green", main = "Histogram of mpg in Mtcars Dataset", breaks = 5)
mtcars$binary <- ifelse(mtcars$mpg > 20, 1,0)
head(mtcars[,c(1,12)])
rows <- sample(1:nrow(mtcars), nrow(mtcars) * .7)
train <- mtcars[rows,]
test <- mtcars[-rows,]
nn <- neuralnet(binary ~ ., data = train,
hidden = 3,
act.fct = "logistic", linear.output = F)
nn$net.result
plot(nn)
predict <- compute(nn, test)
predict$net.result
test.results <- ifelse(predict$net.result > .5, 1,0)
test.results
table(test.results, test$binary)
serve_site()
library(neuralnet)
View(p)
p
plot(nn$linear.output)
plot(nn)
function <- function(x){
b-c^2
}
x <- function(x){
print(1+2)
}
x(3)
x(5)
x <- function(x){
print(1+x)
}
x(4)
x(9)
